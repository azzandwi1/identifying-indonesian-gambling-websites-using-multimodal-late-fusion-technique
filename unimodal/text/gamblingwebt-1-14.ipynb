{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":11661173,"sourceType":"datasetVersion","datasetId":7318121},{"sourceId":11735476,"sourceType":"datasetVersion","datasetId":7367235},{"sourceId":11735580,"sourceType":"datasetVersion","datasetId":7367308}],"dockerImageVersionId":31011,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nimport numpy as np\nimport pandas as pd\nimport random\nimport copy\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.utils import shuffle\n\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import Dataset, DataLoader\nimport torch.optim as optim\n\nfrom PIL import Image\nimport torchvision.models as models\nimport torchvision.transforms as transforms\n\nimport re\nfrom collections import Counter\nfrom transformers import (\n    AutoConfig,\n    AutoTokenizer, \n    AutoModel,\n    AutoModelForSequenceClassification, \n    TrainingArguments, \n    Trainer, \n    DataCollatorWithPadding,\n    get_scheduler\n)\n\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report\nfrom tqdm import tqdm\nimport time\nimport matplotlib.pyplot as plt\nimport warnings\nwarnings.filterwarnings(\"ignore\")","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-05-09T04:15:20.674997Z","iopub.execute_input":"2025-05-09T04:15:20.675245Z","iopub.status.idle":"2025-05-09T04:15:47.971182Z","shell.execute_reply.started":"2025-05-09T04:15:20.675224Z","shell.execute_reply":"2025-05-09T04:15:47.970588Z"}},"outputs":[{"name":"stderr","text":"2025-05-09 04:15:34.932585: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1746764135.114722      31 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1746764135.166805      31 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-09T04:15:47.972118Z","iopub.execute_input":"2025-05-09T04:15:47.972694Z","iopub.status.idle":"2025-05-09T04:15:47.976376Z","shell.execute_reply.started":"2025-05-09T04:15:47.972673Z","shell.execute_reply":"2025-05-09T04:15:47.975658Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"seed = 42\ndef seed_everything(seed):\n    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n    random.seed(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.cuda.manual_seed_all(seed)\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = False\n\nseed_everything(42)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-09T04:15:47.977067Z","iopub.execute_input":"2025-05-09T04:15:47.977267Z","iopub.status.idle":"2025-05-09T04:15:48.004311Z","shell.execute_reply.started":"2025-05-09T04:15:47.977251Z","shell.execute_reply":"2025-05-09T04:15:48.003675Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"# Generator untuk DataLoader\ng = torch.Generator()\ng.manual_seed(seed)\n\ndef seed_worker(worker_id):\n    \"\"\"Fungsi untuk memastikan setiap worker memiliki seed yang sama\"\"\"\n    worker_seed = torch.initial_seed() % 2**32\n    np.random.seed(worker_seed)\n    random.seed(worker_seed)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-09T04:15:48.006022Z","iopub.execute_input":"2025-05-09T04:15:48.006228Z","iopub.status.idle":"2025-05-09T04:15:48.016406Z","shell.execute_reply.started":"2025-05-09T04:15:48.006212Z","shell.execute_reply":"2025-05-09T04:15:48.015746Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"# Paths\ntrain_csv_path = '/kaggle/input/situsjudiid-txt4-2/text/train_data.csv'\ntest_csv_path = '/kaggle/input/situsjudiid-txt4-2/text/test_data.csv'","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-09T04:15:48.017174Z","iopub.execute_input":"2025-05-09T04:15:48.017426Z","iopub.status.idle":"2025-05-09T04:15:48.030973Z","shell.execute_reply.started":"2025-05-09T04:15:48.017402Z","shell.execute_reply":"2025-05-09T04:15:48.030236Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"# Load data\ntrain_df = pd.read_csv(train_csv_path)\ntest_df = pd.read_csv(test_csv_path)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-09T04:15:48.031708Z","iopub.execute_input":"2025-05-09T04:15:48.031941Z","iopub.status.idle":"2025-05-09T04:15:48.152261Z","shell.execute_reply.started":"2025-05-09T04:15:48.031924Z","shell.execute_reply":"2025-05-09T04:15:48.151677Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"# Drop missing values\nprint('Missing values in Train:\\n', train_df.isna().sum())\ntrain_df = train_df.dropna()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-09T04:15:48.152899Z","iopub.execute_input":"2025-05-09T04:15:48.153080Z","iopub.status.idle":"2025-05-09T04:15:48.164362Z","shell.execute_reply.started":"2025-05-09T04:15:48.153065Z","shell.execute_reply":"2025-05-09T04:15:48.163589Z"}},"outputs":[{"name":"stdout","text":"Missing values in Train:\n File Name           0\nExtracted Text    132\nClass               0\ndtype: int64\n","output_type":"stream"}],"execution_count":7},{"cell_type":"code","source":"import re\nimport pandas as pd\nfrom collections import Counter\n\ndef clean_texts(texts):\n    temp_text = []\n    all_words = []\n\n    # Kata 1â€“2 huruf yang penting dan tidak boleh dihapus\n    exceptions = {\n        \"di\", \"ke\", \"ya\"\n    }\n\n    for text in texts:\n\n        # ----- BASIC CLEANING -----\n        text = re.sub(r\"http\\S+\", \"\", text)  # Hapus URL\n        text = re.sub(r\"\\n\", \" \", text)  # Ganti newline dengan spasi\n        text = re.sub(r\"[^a-zA-Z']\", \" \", text)  # Hanya sisakan huruf dan apostrof\n        text = re.sub(r\"\\s{2,}\", \" \", text).strip().lower()  # Hapus spasi ganda, ubah ke lowercase\n\n        # ----- FILTERING -----\n        words = text.split()\n        filtered_words = [\n            w for w in words\n            if (len(w) > 2 or w in exceptions)  # Simpan kata >2 huruf atau ada di exceptions\n        ]\n        text = ' '.join(filtered_words)\n\n        # ----- REMOVE UNWANTED PATTERNS -----\n        text = re.sub(r'\\b[aeiou]+\\b', '', text)  # Hapus kata semua vokal (panjang berapa pun)\n        text = re.sub(r'\\b[^aeiou\\s]+\\b', '', text)  # Hapus kata semua konsonan (panjang berapa pun)\n        text = re.sub(r'\\b\\w{20,}\\b', '', text)  # Hapus kata sangat panjang (â‰¥20 huruf)\n        text = re.sub(r'\\s+', ' ', text).strip()  # Bersihkan spasi ekstra\n\n        temp_text.append(text)  # Simpan teks yang sudah dibersihkan\n        all_words.extend(text.split())  # Simpan semua kata dari semua teks untuk hitung frekuensi global\n\n    # Hitung frekuensi kata\n    word_counts = Counter(all_words)\n    rare_words = {word for word, count in word_counts.items() if count == 1}  # Kata yang muncul 1x\n\n    # Hapus kata yang jarang muncul\n    final_texts = []\n    for text in temp_text:\n        words = text.split()\n        cleaned_words = [word for word in words if word not in rare_words]\n        final_texts.append(\" \".join(cleaned_words))\n\n    return final_texts","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-09T04:15:48.165159Z","iopub.execute_input":"2025-05-09T04:15:48.165460Z","iopub.status.idle":"2025-05-09T04:15:48.177103Z","shell.execute_reply.started":"2025-05-09T04:15:48.165412Z","shell.execute_reply":"2025-05-09T04:15:48.176490Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"# import re\n# import pandas as pd\n# from collections import Counter\n\n# def clean_texts(texts):\n#     temp_text = []\n#     all_words = []\n\n#     for text in texts:\n\n#         # ----- BASIC CLEANING -----\n#         text = re.sub(r\"http\\S+\", \"\", text)  # Hapus URL\n#         text = re.sub(r\"\\n\", \" \", text)  # Ganti newline dengan spasi\n#         text = re.sub(r\"[^a-zA-Z']\", \" \", text)  # Hanya sisakan huruf dan apostrof\n#         text = re.sub(r\"\\s{2,}\", \" \", text).strip().lower()  # Hapus spasi ganda, ubah ke lowercase\n\n#         # ----- REMOVE UNWANTED WORD PATTERNS -----\n#         text = re.sub(r'\\b\\w{1}\\b', '', text)  # Hapus kata 1 huruf\n#         text = re.sub(r'\\b[aeiou]{2,4}\\b', '', text)  # Hapus kata semua vokal 2-4 huruf\n#         text = re.sub(r'\\b[^aeiou\\s]{2,4}\\b', '', text)  # Hapus kata semua konsonan 2-4 huruf\n#         text = re.sub(r'\\b\\w{20,}\\b', '', text)  # Hapus kata sangat panjang (â‰¥20 huruf)\n#         text = re.sub(r'\\s+', ' ', text).strip()  # Bersihkan spasi ekstra\n\n#         temp_text.append(text)  # Simpan teks yang sudah dibersihkan\n#         all_words.extend(text.split())  # Simpan semua kata dari semua teks untuk hitung frekuensi global\n\n#     # Hitung frekuensi kata\n#     word_counts = Counter(all_words)\n#     rare_words = {word for word, count in word_counts.items() if count == 1}  # Kata yang muncul 1x\n\n#     # Hapus kata yang jarang muncul\n#     final_texts = []\n#     for text in temp_text:\n#         words = text.split()\n#         cleaned_words = [word for word in words if word not in rare_words]\n#         final_texts.append(\" \".join(cleaned_words))\n\n#     return final_texts","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-09T04:15:48.177830Z","iopub.execute_input":"2025-05-09T04:15:48.178052Z","iopub.status.idle":"2025-05-09T04:15:48.194396Z","shell.execute_reply.started":"2025-05-09T04:15:48.178032Z","shell.execute_reply":"2025-05-09T04:15:48.193835Z"}},"outputs":[],"execution_count":9},{"cell_type":"code","source":"# # Clean text\n# train_df['cleaned_text'] = clean_texts(train_df['Extracted Text'])\n# test_df['cleaned_text'] = clean_texts(test_df['Extracted Text'])\n# Gabungkan teks dari train dan test sementara\ncombined_texts = pd.concat([train_df['Extracted Text'], test_df['Extracted Text']], ignore_index=True)\n\n# Bersihkan semua teks gabungan\ncleaned_all = clean_texts(combined_texts)\n\n# Bagi kembali hasil cleaned text ke dalam train dan test\ntrain_df['cleaned_text'] = cleaned_all[:len(train_df)]\ntest_df['cleaned_text'] = cleaned_all[len(train_df):]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-09T04:15:48.196601Z","iopub.execute_input":"2025-05-09T04:15:48.196813Z","iopub.status.idle":"2025-05-09T04:15:49.009428Z","shell.execute_reply.started":"2025-05-09T04:15:48.196799Z","shell.execute_reply":"2025-05-09T04:15:49.008664Z"}},"outputs":[],"execution_count":10},{"cell_type":"code","source":"# Drop rows with less than 5 words\ntrain_df = train_df[train_df['cleaned_text'].apply(lambda x: len(str(x).split()) >= 5)]\ntest_df = test_df[test_df['cleaned_text'].apply(lambda x: len(str(x).split()) >= 5)]\ntrain_df","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-09T04:15:49.010129Z","iopub.execute_input":"2025-05-09T04:15:49.010333Z","iopub.status.idle":"2025-05-09T04:15:49.050173Z","shell.execute_reply.started":"2025-05-09T04:15:49.010317Z","shell.execute_reply":"2025-05-09T04:15:49.049593Z"}},"outputs":[{"execution_count":11,"output_type":"execute_result","data":{"text/plain":"                     File Name  \\\n0     0156726.slotslou.sbs.png   \n1      015eaglegaze.online.png   \n2              016-bar.pro.png   \n3             016-good.pro.png   \n4     0164999.slotslou.sbs.png   \n...                        ...   \n7058               zoom.us.png   \n7059          zoom.us_home.png   \n7060         zoom.us_page2.png   \n7061         zoom.us_page4.png   \n7062     zurich.co.id_home.png   \n\n                                         Extracted Text     Class  \\\n0     HOME  jackpotpartycasinoslotsonline â€” login pr...      judi   \n1     r #TOGrt  AFATC Situs Online Gaming Terbaik Un...      judi   \n2     \"â‚¬! INFO TERKINI: (TOGEL ONLINE TERBESAR DAN T...      judi   \n3     Sa TIPE Nomor Ab sernncan 2 Sa HADIAHTOGEL â€”â€” ...      judi   \n4     bet 365 keluaran togel hongkong kemarin  HOME ...      judi   \n...                                                 ...       ...   \n7058  O cari Dukungan 1.888.799.9666 Minta Demo Berg...  non-judi   \n7059  (O Cari Dukungan 1888.7990.9666 Minta Demo  ZO...  non-judi   \n7060  Dukungan  666 Minta Demo Bergabung  stv Masuk ...  non-judi   \n7061  ZOOM  z00m  Work Transformation Summit APAC  F...  non-judi   \n7062  5 Menu KL HubungiKami  #BarengJadiLebih â€” OA  ...  non-judi   \n\n                                           cleaned_text  \n0                       home login bigwin jackpot codes  \n1     situs online gaming terbaik untuk gamers langs...  \n2     info terkini togel online terbesar dan terpeca...  \n3           tipe nomor mimpi imei referral hubungi kami  \n4     bet keluaran togel hongkong kemarin home apk t...  \n...                                                 ...  \n7058  cari dukungan minta demo bergabung host masuk ...  \n7059  cari dukungan minta demo zoom rapat anda denga...  \n7060  dukungan minta demo bergabung masuk hubungi pe...  \n7061  zoom work transformation summit from efficienc...  \n7062  menu hubungikami zurich this content available...  \n\n[6578 rows x 4 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>File Name</th>\n      <th>Extracted Text</th>\n      <th>Class</th>\n      <th>cleaned_text</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0156726.slotslou.sbs.png</td>\n      <td>HOME  jackpotpartycasinoslotsonline â€” login pr...</td>\n      <td>judi</td>\n      <td>home login bigwin jackpot codes</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>015eaglegaze.online.png</td>\n      <td>r #TOGrt  AFATC Situs Online Gaming Terbaik Un...</td>\n      <td>judi</td>\n      <td>situs online gaming terbaik untuk gamers langs...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>016-bar.pro.png</td>\n      <td>\"â‚¬! INFO TERKINI: (TOGEL ONLINE TERBESAR DAN T...</td>\n      <td>judi</td>\n      <td>info terkini togel online terbesar dan terpeca...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>016-good.pro.png</td>\n      <td>Sa TIPE Nomor Ab sernncan 2 Sa HADIAHTOGEL â€”â€” ...</td>\n      <td>judi</td>\n      <td>tipe nomor mimpi imei referral hubungi kami</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0164999.slotslou.sbs.png</td>\n      <td>bet 365 keluaran togel hongkong kemarin  HOME ...</td>\n      <td>judi</td>\n      <td>bet keluaran togel hongkong kemarin home apk t...</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>7058</th>\n      <td>zoom.us.png</td>\n      <td>O cari Dukungan 1.888.799.9666 Minta Demo Berg...</td>\n      <td>non-judi</td>\n      <td>cari dukungan minta demo bergabung host masuk ...</td>\n    </tr>\n    <tr>\n      <th>7059</th>\n      <td>zoom.us_home.png</td>\n      <td>(O Cari Dukungan 1888.7990.9666 Minta Demo  ZO...</td>\n      <td>non-judi</td>\n      <td>cari dukungan minta demo zoom rapat anda denga...</td>\n    </tr>\n    <tr>\n      <th>7060</th>\n      <td>zoom.us_page2.png</td>\n      <td>Dukungan  666 Minta Demo Bergabung  stv Masuk ...</td>\n      <td>non-judi</td>\n      <td>dukungan minta demo bergabung masuk hubungi pe...</td>\n    </tr>\n    <tr>\n      <th>7061</th>\n      <td>zoom.us_page4.png</td>\n      <td>ZOOM  z00m  Work Transformation Summit APAC  F...</td>\n      <td>non-judi</td>\n      <td>zoom work transformation summit from efficienc...</td>\n    </tr>\n    <tr>\n      <th>7062</th>\n      <td>zurich.co.id_home.png</td>\n      <td>5 Menu KL HubungiKami  #BarengJadiLebih â€” OA  ...</td>\n      <td>non-judi</td>\n      <td>menu hubungikami zurich this content available...</td>\n    </tr>\n  </tbody>\n</table>\n<p>6578 rows Ã— 4 columns</p>\n</div>"},"metadata":{}}],"execution_count":11},{"cell_type":"code","source":"# Cek jumlah duplikasi sebelum dihapus\nprint(\"Duplikasi di train:\", train_df.duplicated(subset='cleaned_text').sum())\nprint(\"Duplikasi di test :\", test_df.duplicated(subset='cleaned_text').sum())\n\n# Hapus duplikasi berdasarkan cleaned_text\ntrain_df = train_df.drop_duplicates(subset='cleaned_text').reset_index(drop=True)\ntest_df = test_df.drop_duplicates(subset='cleaned_text').reset_index(drop=True)\n\n# Cek ulang setelah pembersihan\nprint(\"Setelah dihapus:\")\nprint(\"Train:\", len(train_df), \"baris\")\nprint(\"Test :\", len(test_df), \"baris\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-09T04:15:49.050844Z","iopub.execute_input":"2025-05-09T04:15:49.051059Z","iopub.status.idle":"2025-05-09T04:15:49.063966Z","shell.execute_reply.started":"2025-05-09T04:15:49.051032Z","shell.execute_reply":"2025-05-09T04:15:49.063297Z"}},"outputs":[{"name":"stdout","text":"Duplikasi di train: 37\nDuplikasi di test : 0\nSetelah dihapus:\nTrain: 6541 baris\nTest : 700 baris\n","output_type":"stream"}],"execution_count":12},{"cell_type":"code","source":"# Print jumlah per kelas\nprint(\"Distribusi label di Train set:\")\nprint(train_df['Class'].value_counts(), '\\n')\n\nprint(\"Distribusi label di Test set:\")\nprint(test_df['Class'].value_counts())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-09T04:15:49.064866Z","iopub.execute_input":"2025-05-09T04:15:49.065109Z","iopub.status.idle":"2025-05-09T04:15:49.078279Z","shell.execute_reply.started":"2025-05-09T04:15:49.065092Z","shell.execute_reply":"2025-05-09T04:15:49.077767Z"}},"outputs":[{"name":"stdout","text":"Distribusi label di Train set:\nClass\nnon-judi    3969\njudi        2572\nName: count, dtype: int64 \n\nDistribusi label di Test set:\nClass\njudi        350\nnon-judi    350\nName: count, dtype: int64\n","output_type":"stream"}],"execution_count":13},{"cell_type":"code","source":"from sklearn.utils import resample\n\n# Pisahkan data berdasarkan kelas\ntrain_judi = train_df[train_df['Class'] == 'judi']\ntrain_nonjudi = train_df[train_df['Class'] == 'non-judi']\n\n# Undersampling kelas mayoritas (non-judi) agar jumlahnya sama dengan kelas judi\ntrain_nonjudi_undersampled = resample(train_nonjudi,\n                                      replace=False,      # tanpa duplikasi\n                                      n_samples=len(train_judi),  # samakan jumlahnya dengan kelas minoritas\n                                      random_state=42)    # untuk replikasi hasil\n\n# Gabungkan kembali data yang sudah diundersample\ntrain_df_balanced = pd.concat([train_judi, train_nonjudi_undersampled])\n\n# Cek distribusi baru\nprint(\"Distribusi label setelah undersampling:\")\nprint(train_df_balanced['Class'].value_counts())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-09T04:15:49.078899Z","iopub.execute_input":"2025-05-09T04:15:49.079138Z","iopub.status.idle":"2025-05-09T04:15:49.099297Z","shell.execute_reply.started":"2025-05-09T04:15:49.079122Z","shell.execute_reply":"2025-05-09T04:15:49.098682Z"}},"outputs":[{"name":"stdout","text":"Distribusi label setelah undersampling:\nClass\njudi        2572\nnon-judi    2572\nName: count, dtype: int64\n","output_type":"stream"}],"execution_count":14},{"cell_type":"code","source":"label_map = {\n    \"non-judi\": 0,\n    \"judi\": 1\n}\n\ntrain_df_balanced['label'] = train_df_balanced['Class'].map(label_map)\ntest_df['label'] = test_df['Class'].map(label_map)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-09T04:15:49.100327Z","iopub.execute_input":"2025-05-09T04:15:49.100564Z","iopub.status.idle":"2025-05-09T04:15:49.118191Z","shell.execute_reply.started":"2025-05-09T04:15:49.100549Z","shell.execute_reply":"2025-05-09T04:15:49.117681Z"}},"outputs":[],"execution_count":15},{"cell_type":"code","source":"traindf, validdf = train_test_split(\n    train_df_balanced, test_size=0.2, stratify=train_df_balanced['label'], random_state=42\n)\n\nprint(f\"Jumlah data train: {len(traindf)}\")\nprint(f\"Jumlah data valid: {len(validdf)}\")\nprint(f\"Jumlah data test: {len(test_df)}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-09T04:15:49.118855Z","iopub.execute_input":"2025-05-09T04:15:49.119073Z","iopub.status.idle":"2025-05-09T04:15:49.136725Z","shell.execute_reply.started":"2025-05-09T04:15:49.119050Z","shell.execute_reply":"2025-05-09T04:15:49.136030Z"}},"outputs":[{"name":"stdout","text":"Jumlah data train: 4115\nJumlah data valid: 1029\nJumlah data test: 700\n","output_type":"stream"}],"execution_count":16},{"cell_type":"code","source":"# Print jumlah per kelas\nprint(\"Distribusi label di Train set:\")\nprint(traindf['label'].value_counts(), '\\n')\n\nprint(\"Distribusi label di Validation set:\")\nprint(validdf['label'].value_counts(), '\\n')\n\nprint(\"Distribusi label di Test set:\")\nprint(test_df['label'].value_counts())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-09T04:15:49.137508Z","iopub.execute_input":"2025-05-09T04:15:49.137780Z","iopub.status.idle":"2025-05-09T04:15:49.151217Z","shell.execute_reply.started":"2025-05-09T04:15:49.137757Z","shell.execute_reply":"2025-05-09T04:15:49.150659Z"}},"outputs":[{"name":"stdout","text":"Distribusi label di Train set:\nlabel\n1    2058\n0    2057\nName: count, dtype: int64 \n\nDistribusi label di Validation set:\nlabel\n0    515\n1    514\nName: count, dtype: int64 \n\nDistribusi label di Test set:\nlabel\n1    350\n0    350\nName: count, dtype: int64\n","output_type":"stream"}],"execution_count":17},{"cell_type":"code","source":"class CustomDataset(Dataset):\n    def __init__(self, dataframe, tokenizer, max_length):\n        self.dataframe = dataframe.reset_index(drop=True)\n        self.tokenizer = tokenizer\n        self.max_length = max_length\n\n    def __len__(self):\n        return len(self.dataframe)\n\n    def __getitem__(self, index):\n        row = self.dataframe.iloc[index]\n        label = row['label']\n        text = str(row['cleaned_text'])\n\n        encoding = self.tokenizer.encode_plus(\n            text,\n            add_special_tokens=True,\n            max_length=self.max_length,\n            truncation=True,\n            return_tensors='pt',\n            padding='max_length'\n        )\n\n        input_ids = encoding['input_ids'].flatten()\n        attention_mask = encoding['attention_mask'].flatten()\n\n        return {\n            'input_ids': input_ids,\n            'attention_mask': attention_mask,\n            'label': torch.tensor(label, dtype=torch.long)\n        }\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-09T04:15:49.151948Z","iopub.execute_input":"2025-05-09T04:15:49.152203Z","iopub.status.idle":"2025-05-09T04:15:49.166943Z","shell.execute_reply.started":"2025-05-09T04:15:49.152182Z","shell.execute_reply":"2025-05-09T04:15:49.166303Z"}},"outputs":[],"execution_count":18},{"cell_type":"code","source":"# Tokenizer\ntokenizer = AutoTokenizer.from_pretrained('indobenchmark/indobert-base-p1')\nmax_length = 128\n\n# Dataset\ntrain_dataset = CustomDataset(traindf, tokenizer, max_length)\nvalid_dataset = CustomDataset(validdf, tokenizer, max_length)\ntest_dataset = CustomDataset(test_df, tokenizer, max_length)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-09T04:15:49.167552Z","iopub.execute_input":"2025-05-09T04:15:49.167750Z","iopub.status.idle":"2025-05-09T04:15:50.357833Z","shell.execute_reply.started":"2025-05-09T04:15:49.167735Z","shell.execute_reply":"2025-05-09T04:15:50.357244Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/2.00 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4d8965d4b7524eb2bd0d409a9da155fc"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/1.53k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"872a328711a646fc8a784272311b2f46"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt:   0%|          | 0.00/229k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8e5e1e22000848639b337e6f020635c5"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e63bcac7eb274a04aa95e2bcd5bc57f1"}},"metadata":{}}],"execution_count":19},{"cell_type":"code","source":"def run_experiment(batch_size, learning_rate, num_epochs, device):\n    experiment_name = f\"bs{batch_size}_lr{learning_rate}_ep{num_epochs}\"\n    print(f\"\\n===== STARTING EXPERIMENT: {experiment_name} =====\\n\")\n\n    # Inisialisasi DataLoader\n    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, \n                              num_workers=0, worker_init_fn=seed_worker, generator=g)\n    valid_loader = DataLoader(valid_dataset, batch_size=batch_size, shuffle=False, \n                              num_workers=0, worker_init_fn=seed_worker, generator=g)\n    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, \n                             num_workers=0, worker_init_fn=seed_worker, generator=g)\n\n    # Model\n    text_model = AutoModelForSequenceClassification.from_pretrained('indobenchmark/indobert-base-p1', num_labels=1)\n    text_model.to(device)\n    is_parallel = False\n    if torch.cuda.device_count() > 1:\n        print(f\"Using {torch.cuda.device_count()} GPUs!\")\n        text_model = nn.DataParallel(text_model)\n        is_parallel = True\n\n    optimizer = optim.Adam(text_model.parameters(), lr=learning_rate)\n    criterion = nn.BCEWithLogitsLoss()\n\n    train_losses, train_accuracies = [], []\n    valid_losses, valid_accuracies = [], []\n\n    best_val_loss = float('inf')\n    best_model_path = f'best_text_model_{experiment_name}_state_dict.pt'\n\n    # Time tracking\n    training_start_time = time.time()\n\n    for epoch in range(num_epochs):\n        # TRAINING\n        epoch_train_start = time.time()\n        text_model.train()\n        total_loss = 0.0\n        correct = 0\n        total_samples = 0\n\n        for batch_data in tqdm(train_loader, desc=f\"Epoch {epoch+1} Training\"):\n            input_ids = batch_data['input_ids'].to(device)\n            attention_mask = batch_data['attention_mask'].to(device)\n            labels = batch_data['label'].float().to(device)\n\n            optimizer.zero_grad()\n            outputs = text_model(input_ids=input_ids, attention_mask=attention_mask).logits.squeeze(1)\n            loss = criterion(outputs, labels)\n            loss.backward()\n            optimizer.step()\n\n            total_loss += loss.item()\n            predicted = (torch.sigmoid(outputs) > 0.5).long()\n            correct += (predicted == labels).sum().item()\n            total_samples += labels.size(0)\n\n        epoch_loss = total_loss / len(train_loader)\n        epoch_accuracy = correct / total_samples\n        train_losses.append(epoch_loss)\n        train_accuracies.append(epoch_accuracy)\n        epoch_train_duration = time.time() - epoch_train_start\n\n        # VALIDATION\n        epoch_val_start = time.time()\n        text_model.eval()\n        valid_loss = 0.0\n        valid_correct = 0\n        valid_total_samples = 0\n\n        with torch.no_grad():\n            for batch_data in valid_loader:\n                input_ids = batch_data['input_ids'].to(device)\n                attention_mask = batch_data['attention_mask'].to(device)\n                labels = batch_data['label'].float().to(device)\n\n                outputs = text_model(input_ids=input_ids, attention_mask=attention_mask).logits.squeeze(1)\n                loss = criterion(outputs, labels)\n                valid_loss += loss.item()\n\n                predicted = (torch.sigmoid(outputs) > 0.5).long()\n                valid_correct += (predicted == labels).sum().item()\n                valid_total_samples += labels.size(0)\n\n        valid_epoch_loss = valid_loss / len(valid_loader)\n        valid_epoch_accuracy = valid_correct / valid_total_samples\n        valid_losses.append(valid_epoch_loss)\n        valid_accuracies.append(valid_epoch_accuracy)\n        epoch_val_duration = time.time() - epoch_val_start\n\n        print(f\"Epoch [{epoch+1}/{num_epochs}], Train Loss: {epoch_loss:.4f}, Acc: {epoch_accuracy:.4f}, Time: {epoch_train_duration:.2f}s\")\n        print(f\"Epoch [{epoch+1}/{num_epochs}], Val   Loss: {valid_epoch_loss:.4f}, Acc: {valid_epoch_accuracy:.4f}, Time: {epoch_val_duration:.2f}s\")\n\n        if valid_epoch_loss < best_val_loss:\n            best_val_loss = valid_epoch_loss\n            torch.save(text_model.module.state_dict() if is_parallel else text_model.state_dict(), best_model_path)\n            print(f\"New best model saved at epoch {epoch+1} with val loss: {valid_epoch_loss:.4f}\")\n\n    total_training_duration = time.time() - training_start_time\n    print(f\"\\nâœ… Total training + validation time: {total_training_duration:.2f} seconds\\n\")\n\n    # Load best model\n    best_model = AutoModelForSequenceClassification.from_pretrained('indobenchmark/indobert-base-p1', num_labels=1)\n    best_model.load_state_dict(torch.load(best_model_path))\n    best_model.to(device)\n\n    # EVALUATION\n    test_start_time = time.time()\n    metrics = evaluate_text_model(best_model, test_loader, device)\n    test_duration = time.time() - test_start_time\n    print(f\"ðŸ§ª Test time: {test_duration:.2f} seconds\")\n\n    return {\n        'experiment_name': experiment_name,\n        'metrics': metrics,\n        'model_path': best_model_path,\n        'train_losses': train_losses,\n        'train_accuracies': train_accuracies,\n        'valid_losses': valid_losses,\n        'valid_accuracies': valid_accuracies,\n        'best_val_loss': best_val_loss,\n        'best_epoch': np.argmin(valid_losses) + 1,\n        'train_time': total_training_duration,\n        'test_time': test_duration,\n    }","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-09T04:15:50.358533Z","iopub.execute_input":"2025-05-09T04:15:50.358760Z","iopub.status.idle":"2025-05-09T04:15:50.372418Z","shell.execute_reply.started":"2025-05-09T04:15:50.358743Z","shell.execute_reply":"2025-05-09T04:15:50.371615Z"}},"outputs":[],"execution_count":20},{"cell_type":"code","source":"# Define function to evaluate text model\ndef evaluate_text_model(model, data_loader, device): \n    model.eval()\n    all_preds = []\n    all_labels = []\n\n    with torch.no_grad():\n        for batch_data in tqdm(data_loader, desc=\"Evaluating Text Model\"):\n            input_ids = batch_data['input_ids'].to(device)\n            attention_mask = batch_data['attention_mask'].to(device)\n            labels = batch_data['label'].to(device)\n\n            outputs = model(input_ids=input_ids, attention_mask=attention_mask).logits.squeeze(1)\n            preds = (torch.sigmoid(outputs) > 0.5).cpu().numpy()\n\n            all_preds.extend(preds)\n            all_labels.extend(labels.cpu().numpy())\n    \n    # Calculate metrics\n    metrics = {\n        'Accuracy': accuracy_score(all_labels, all_preds),\n        'Precision': precision_score(all_labels, all_preds),\n        'Recall': recall_score(all_labels, all_preds),\n        'F1 Score': f1_score(all_labels, all_preds)\n    }\n    \n    return metrics","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-09T04:15:50.373058Z","iopub.execute_input":"2025-05-09T04:15:50.373226Z","iopub.status.idle":"2025-05-09T04:15:50.391875Z","shell.execute_reply.started":"2025-05-09T04:15:50.373213Z","shell.execute_reply":"2025-05-09T04:15:50.391103Z"}},"outputs":[],"execution_count":21},{"cell_type":"code","source":"# Define experiments based on the specified hyperparameters\nexperiments = [\n    {'batch_size': bs, 'learning_rate': lr, 'num_epochs': ep}\n    for bs in [16, 32]\n    for lr in [5e-5, 3e-5, 2e-5]\n    for ep in [2, 3, 4]\n]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-09T04:15:50.392582Z","iopub.execute_input":"2025-05-09T04:15:50.392877Z","iopub.status.idle":"2025-05-09T04:15:50.410850Z","shell.execute_reply.started":"2025-05-09T04:15:50.392838Z","shell.execute_reply":"2025-05-09T04:15:50.410227Z"}},"outputs":[],"execution_count":22},{"cell_type":"code","source":"# Run all experiments and collect results\nresults = []\nfor i, exp in enumerate(experiments):\n    print(f\"\\n\\n===== RUNNING EXPERIMENT {i+1}/{len(experiments)} =====\")\n    print(f\"Batch Size: {exp['batch_size']}, Learning Rate: {exp['learning_rate']}, Epochs: {exp['num_epochs']}\")\n    \n    experiment_result = run_experiment(\n        batch_size=exp['batch_size'],\n        learning_rate=exp['learning_rate'],\n        num_epochs=exp['num_epochs'],\n        device=device\n    )\n    \n    results.append(experiment_result)\n    \n    # Print current experiment metrics\n    print(f\"\\n----- Results for Experiment {i+1}: {experiment_result['experiment_name']} -----\")\n    print(f\"Best epoch: {experiment_result['best_epoch']} with validation loss: {experiment_result['best_val_loss']:.4f}\")\n    print(\"Test Metrics:\")\n    for metric_name, value in experiment_result['metrics'].items():\n        print(f\"  {metric_name}: {value:.4f}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-09T04:15:50.411695Z","iopub.execute_input":"2025-05-09T04:15:50.411950Z","iopub.status.idle":"2025-05-09T05:09:10.872184Z","shell.execute_reply.started":"2025-05-09T04:15:50.411928Z","shell.execute_reply":"2025-05-09T05:09:10.871253Z"}},"outputs":[{"name":"stdout","text":"\n\n===== RUNNING EXPERIMENT 1/18 =====\nBatch Size: 16, Learning Rate: 5e-05, Epochs: 2\n\n===== STARTING EXPERIMENT: bs16_lr5e-05_ep2 =====\n\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"pytorch_model.bin:   0%|          | 0.00/498M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"862d805ecf5a46ec8a1640509430d1d7"}},"metadata":{}},{"name":"stderr","text":"Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/498M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"06fb09dd9e47424fb73fcbdf11289212"}},"metadata":{}},{"name":"stderr","text":"\nEpoch 1 Training:   0%|          | 0/258 [00:00<?, ?it/s]\u001b[A\nEpoch 1 Training:   0%|          | 1/258 [00:00<03:25,  1.25it/s]\u001b[A\nEpoch 1 Training:   1%|          | 2/258 [00:01<01:56,  2.19it/s]\u001b[A\nEpoch 1 Training:   1%|          | 3/258 [00:01<01:29,  2.84it/s]\u001b[A\nEpoch 1 Training:   2%|â–         | 4/258 [00:01<01:16,  3.32it/s]\u001b[A\nEpoch 1 Training:   2%|â–         | 5/258 [00:01<01:09,  3.67it/s]\u001b[A\nEpoch 1 Training:   2%|â–         | 6/258 [00:01<01:04,  3.91it/s]\u001b[A\nEpoch 1 Training:   3%|â–Ž         | 7/258 [00:02<01:00,  4.14it/s]\u001b[A\nEpoch 1 Training:   3%|â–Ž         | 8/258 [00:02<00:58,  4.29it/s]\u001b[A\nEpoch 1 Training:   3%|â–Ž         | 9/258 [00:02<00:56,  4.41it/s]\u001b[A\nEpoch 1 Training:   4%|â–         | 10/258 [00:02<00:55,  4.51it/s]\u001b[A\nEpoch 1 Training:   4%|â–         | 11/258 [00:02<00:54,  4.57it/s]\u001b[A\nEpoch 1 Training:   5%|â–         | 12/258 [00:03<00:53,  4.58it/s]\u001b[A\nEpoch 1 Training:   5%|â–Œ         | 13/258 [00:03<00:52,  4.63it/s]\u001b[A\nEpoch 1 Training:   5%|â–Œ         | 14/258 [00:03<00:52,  4.66it/s]\u001b[A\nEpoch 1 Training:   6%|â–Œ         | 15/258 [00:03<00:51,  4.68it/s]\u001b[A\nEpoch 1 Training:   6%|â–Œ         | 16/258 [00:04<00:51,  4.69it/s]\u001b[A\nEpoch 1 Training:   7%|â–‹         | 17/258 [00:04<00:51,  4.71it/s]\u001b[A\nEpoch 1 Training:   7%|â–‹         | 18/258 [00:04<00:50,  4.72it/s]\u001b[A\nEpoch 1 Training:   7%|â–‹         | 19/258 [00:04<00:50,  4.72it/s]\u001b[A\nEpoch 1 Training:   8%|â–Š         | 20/258 [00:04<00:50,  4.73it/s]\u001b[A\nEpoch 1 Training:   8%|â–Š         | 21/258 [00:05<00:50,  4.73it/s]\u001b[A\nEpoch 1 Training:   9%|â–Š         | 22/258 [00:05<00:50,  4.69it/s]\u001b[A\nEpoch 1 Training:   9%|â–‰         | 23/258 [00:05<00:50,  4.68it/s]\u001b[A\nEpoch 1 Training:   9%|â–‰         | 24/258 [00:05<00:50,  4.64it/s]\u001b[A\nEpoch 1 Training:  10%|â–‰         | 25/258 [00:05<00:49,  4.68it/s]\u001b[A\nEpoch 1 Training:  10%|â–ˆ         | 26/258 [00:06<00:49,  4.69it/s]\u001b[A\nEpoch 1 Training:  10%|â–ˆ         | 27/258 [00:06<00:49,  4.70it/s]\u001b[A\nEpoch 1 Training:  11%|â–ˆ         | 28/258 [00:06<00:48,  4.71it/s]\u001b[A\nEpoch 1 Training:  11%|â–ˆ         | 29/258 [00:06<00:48,  4.71it/s]\u001b[A\nEpoch 1 Training:  12%|â–ˆâ–        | 30/258 [00:07<00:48,  4.71it/s]\u001b[A\nEpoch 1 Training:  12%|â–ˆâ–        | 31/258 [00:07<00:48,  4.71it/s]\u001b[A\nEpoch 1 Training:  12%|â–ˆâ–        | 32/258 [00:07<00:47,  4.71it/s]\u001b[A\nEpoch 1 Training:  13%|â–ˆâ–Ž        | 33/258 [00:07<00:47,  4.70it/s]\u001b[A\nEpoch 1 Training:  13%|â–ˆâ–Ž        | 34/258 [00:07<00:47,  4.71it/s]\u001b[A\nEpoch 1 Training:  14%|â–ˆâ–Ž        | 35/258 [00:08<00:47,  4.69it/s]\u001b[A\nEpoch 1 Training:  14%|â–ˆâ–        | 36/258 [00:08<00:47,  4.71it/s]\u001b[A\nEpoch 1 Training:  14%|â–ˆâ–        | 37/258 [00:08<00:46,  4.72it/s]\u001b[A\nEpoch 1 Training:  15%|â–ˆâ–        | 38/258 [00:08<00:46,  4.74it/s]\u001b[A\nEpoch 1 Training:  15%|â–ˆâ–Œ        | 39/258 [00:08<00:46,  4.72it/s]\u001b[A\nEpoch 1 Training:  16%|â–ˆâ–Œ        | 40/258 [00:09<00:46,  4.73it/s]\u001b[A\nEpoch 1 Training:  16%|â–ˆâ–Œ        | 41/258 [00:09<00:46,  4.72it/s]\u001b[A\nEpoch 1 Training:  16%|â–ˆâ–‹        | 42/258 [00:09<00:45,  4.72it/s]\u001b[A\nEpoch 1 Training:  17%|â–ˆâ–‹        | 43/258 [00:09<00:45,  4.73it/s]\u001b[A\nEpoch 1 Training:  17%|â–ˆâ–‹        | 44/258 [00:09<00:45,  4.73it/s]\u001b[A\nEpoch 1 Training:  17%|â–ˆâ–‹        | 45/258 [00:10<00:44,  4.74it/s]\u001b[A\nEpoch 1 Training:  18%|â–ˆâ–Š        | 46/258 [00:10<00:44,  4.74it/s]\u001b[A\nEpoch 1 Training:  18%|â–ˆâ–Š        | 47/258 [00:10<00:44,  4.74it/s]\u001b[A\nEpoch 1 Training:  19%|â–ˆâ–Š        | 48/258 [00:10<00:44,  4.74it/s]\u001b[A\nEpoch 1 Training:  19%|â–ˆâ–‰        | 49/258 [00:11<00:44,  4.73it/s]\u001b[A\nEpoch 1 Training:  19%|â–ˆâ–‰        | 50/258 [00:11<00:43,  4.75it/s]\u001b[A\nEpoch 1 Training:  20%|â–ˆâ–‰        | 51/258 [00:11<00:43,  4.75it/s]\u001b[A\nEpoch 1 Training:  20%|â–ˆâ–ˆ        | 52/258 [00:11<00:43,  4.74it/s]\u001b[A\nEpoch 1 Training:  21%|â–ˆâ–ˆ        | 53/258 [00:11<00:43,  4.75it/s]\u001b[A\nEpoch 1 Training:  21%|â–ˆâ–ˆ        | 54/258 [00:12<00:42,  4.75it/s]\u001b[A\nEpoch 1 Training:  21%|â–ˆâ–ˆâ–       | 55/258 [00:12<00:42,  4.74it/s]\u001b[A\nEpoch 1 Training:  22%|â–ˆâ–ˆâ–       | 56/258 [00:12<00:42,  4.75it/s]\u001b[A\nEpoch 1 Training:  22%|â–ˆâ–ˆâ–       | 57/258 [00:12<00:42,  4.74it/s]\u001b[A\nEpoch 1 Training:  22%|â–ˆâ–ˆâ–       | 58/258 [00:12<00:42,  4.75it/s]\u001b[A\nEpoch 1 Training:  23%|â–ˆâ–ˆâ–Ž       | 59/258 [00:13<00:42,  4.74it/s]\u001b[A\nEpoch 1 Training:  23%|â–ˆâ–ˆâ–Ž       | 60/258 [00:13<00:41,  4.74it/s]\u001b[A\nEpoch 1 Training:  24%|â–ˆâ–ˆâ–Ž       | 61/258 [00:13<00:41,  4.74it/s]\u001b[A\nEpoch 1 Training:  24%|â–ˆâ–ˆâ–       | 62/258 [00:13<00:41,  4.75it/s]\u001b[A\nEpoch 1 Training:  24%|â–ˆâ–ˆâ–       | 63/258 [00:13<00:41,  4.75it/s]\u001b[A\nEpoch 1 Training:  25%|â–ˆâ–ˆâ–       | 64/258 [00:14<00:40,  4.74it/s]\u001b[A\nEpoch 1 Training:  25%|â–ˆâ–ˆâ–Œ       | 65/258 [00:14<00:40,  4.74it/s]\u001b[A\nEpoch 1 Training:  26%|â–ˆâ–ˆâ–Œ       | 66/258 [00:14<00:40,  4.73it/s]\u001b[A\nEpoch 1 Training:  26%|â–ˆâ–ˆâ–Œ       | 67/258 [00:14<00:40,  4.73it/s]\u001b[A\nEpoch 1 Training:  26%|â–ˆâ–ˆâ–‹       | 68/258 [00:15<00:40,  4.73it/s]\u001b[A\nEpoch 1 Training:  27%|â–ˆâ–ˆâ–‹       | 69/258 [00:15<00:39,  4.74it/s]\u001b[A\nEpoch 1 Training:  27%|â–ˆâ–ˆâ–‹       | 70/258 [00:15<00:39,  4.74it/s]\u001b[A\nEpoch 1 Training:  28%|â–ˆâ–ˆâ–Š       | 71/258 [00:15<00:39,  4.74it/s]\u001b[A\nEpoch 1 Training:  28%|â–ˆâ–ˆâ–Š       | 72/258 [00:15<00:39,  4.75it/s]\u001b[A\nEpoch 1 Training:  28%|â–ˆâ–ˆâ–Š       | 73/258 [00:16<00:38,  4.75it/s]\u001b[A\nEpoch 1 Training:  29%|â–ˆâ–ˆâ–Š       | 74/258 [00:16<00:38,  4.74it/s]\u001b[A\nEpoch 1 Training:  29%|â–ˆâ–ˆâ–‰       | 75/258 [00:16<00:38,  4.74it/s]\u001b[A\nEpoch 1 Training:  29%|â–ˆâ–ˆâ–‰       | 76/258 [00:16<00:38,  4.73it/s]\u001b[A\nEpoch 1 Training:  30%|â–ˆâ–ˆâ–‰       | 77/258 [00:16<00:38,  4.74it/s]\u001b[A\nEpoch 1 Training:  30%|â–ˆâ–ˆâ–ˆ       | 78/258 [00:17<00:38,  4.73it/s]\u001b[A\nEpoch 1 Training:  31%|â–ˆâ–ˆâ–ˆ       | 79/258 [00:17<00:37,  4.73it/s]\u001b[A\nEpoch 1 Training:  31%|â–ˆâ–ˆâ–ˆ       | 80/258 [00:17<00:37,  4.73it/s]\u001b[A\nEpoch 1 Training:  31%|â–ˆâ–ˆâ–ˆâ–      | 81/258 [00:17<00:37,  4.74it/s]\u001b[A\nEpoch 1 Training:  32%|â–ˆâ–ˆâ–ˆâ–      | 82/258 [00:17<00:37,  4.73it/s]\u001b[A\nEpoch 1 Training:  32%|â–ˆâ–ˆâ–ˆâ–      | 83/258 [00:18<00:37,  4.71it/s]\u001b[A\nEpoch 1 Training:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 84/258 [00:18<00:36,  4.73it/s]\u001b[A\nEpoch 1 Training:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 85/258 [00:18<00:36,  4.73it/s]\u001b[A\nEpoch 1 Training:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 86/258 [00:18<00:36,  4.73it/s]\u001b[A\nEpoch 1 Training:  34%|â–ˆâ–ˆâ–ˆâ–Ž      | 87/258 [00:19<00:36,  4.74it/s]\u001b[A\nEpoch 1 Training:  34%|â–ˆâ–ˆâ–ˆâ–      | 88/258 [00:19<00:35,  4.75it/s]\u001b[A\nEpoch 1 Training:  34%|â–ˆâ–ˆâ–ˆâ–      | 89/258 [00:19<00:35,  4.75it/s]\u001b[A\nEpoch 1 Training:  35%|â–ˆâ–ˆâ–ˆâ–      | 90/258 [00:19<00:35,  4.73it/s]\u001b[A\nEpoch 1 Training:  35%|â–ˆâ–ˆâ–ˆâ–Œ      | 91/258 [00:19<00:35,  4.74it/s]\u001b[A\nEpoch 1 Training:  36%|â–ˆâ–ˆâ–ˆâ–Œ      | 92/258 [00:20<00:35,  4.74it/s]\u001b[A\nEpoch 1 Training:  36%|â–ˆâ–ˆâ–ˆâ–Œ      | 93/258 [00:20<00:34,  4.75it/s]\u001b[A\nEpoch 1 Training:  36%|â–ˆâ–ˆâ–ˆâ–‹      | 94/258 [00:20<00:34,  4.75it/s]\u001b[A\nEpoch 1 Training:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 95/258 [00:20<00:34,  4.74it/s]\u001b[A\nEpoch 1 Training:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 96/258 [00:20<00:34,  4.74it/s]\u001b[A\nEpoch 1 Training:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 97/258 [00:21<00:33,  4.74it/s]\u001b[A\nEpoch 1 Training:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 98/258 [00:21<00:33,  4.73it/s]\u001b[A\nEpoch 1 Training:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 99/258 [00:21<00:33,  4.73it/s]\u001b[A\nEpoch 1 Training:  39%|â–ˆâ–ˆâ–ˆâ–‰      | 100/258 [00:21<00:33,  4.74it/s]\u001b[A\nEpoch 1 Training:  39%|â–ˆâ–ˆâ–ˆâ–‰      | 101/258 [00:22<00:33,  4.74it/s]\u001b[A\nEpoch 1 Training:  40%|â–ˆâ–ˆâ–ˆâ–‰      | 102/258 [00:22<00:32,  4.74it/s]\u001b[A\nEpoch 1 Training:  40%|â–ˆâ–ˆâ–ˆâ–‰      | 103/258 [00:22<00:32,  4.74it/s]\u001b[A\nEpoch 1 Training:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 104/258 [00:22<00:32,  4.74it/s]\u001b[A\nEpoch 1 Training:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 105/258 [00:22<00:32,  4.74it/s]\u001b[A\nEpoch 1 Training:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 106/258 [00:23<00:32,  4.73it/s]\u001b[A\nEpoch 1 Training:  41%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 107/258 [00:23<00:31,  4.72it/s]\u001b[A\nEpoch 1 Training:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 108/258 [00:23<00:31,  4.73it/s]\u001b[A\nEpoch 1 Training:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 109/258 [00:23<00:31,  4.74it/s]\u001b[A\nEpoch 1 Training:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 110/258 [00:23<00:31,  4.73it/s]\u001b[A\nEpoch 1 Training:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 111/258 [00:24<00:31,  4.74it/s]\u001b[A\nEpoch 1 Training:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 112/258 [00:24<00:30,  4.74it/s]\u001b[A\nEpoch 1 Training:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 113/258 [00:24<00:30,  4.73it/s]\u001b[A\nEpoch 1 Training:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 114/258 [00:24<00:30,  4.74it/s]\u001b[A\nEpoch 1 Training:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 115/258 [00:24<00:30,  4.73it/s]\u001b[A\nEpoch 1 Training:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 116/258 [00:25<00:29,  4.73it/s]\u001b[A\nEpoch 1 Training:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 117/258 [00:25<00:29,  4.73it/s]\u001b[A\nEpoch 1 Training:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 118/258 [00:25<00:29,  4.73it/s]\u001b[A\nEpoch 1 Training:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 119/258 [00:25<00:29,  4.73it/s]\u001b[A\nEpoch 1 Training:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 120/258 [00:26<00:29,  4.73it/s]\u001b[A\nEpoch 1 Training:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 121/258 [00:26<00:28,  4.73it/s]\u001b[A\nEpoch 1 Training:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 122/258 [00:26<00:28,  4.74it/s]\u001b[A\nEpoch 1 Training:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 123/258 [00:26<00:28,  4.74it/s]\u001b[A\nEpoch 1 Training:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 124/258 [00:26<00:28,  4.75it/s]\u001b[A\nEpoch 1 Training:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 125/258 [00:27<00:28,  4.74it/s]\u001b[A\nEpoch 1 Training:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 126/258 [00:27<00:27,  4.74it/s]\u001b[A\nEpoch 1 Training:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 127/258 [00:27<00:27,  4.74it/s]\u001b[A\nEpoch 1 Training:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 128/258 [00:27<00:27,  4.74it/s]\u001b[A\nEpoch 1 Training:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 129/258 [00:27<00:27,  4.74it/s]\u001b[A\nEpoch 1 Training:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 130/258 [00:28<00:27,  4.73it/s]\u001b[A\nEpoch 1 Training:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 131/258 [00:28<00:26,  4.74it/s]\u001b[A\nEpoch 1 Training:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 132/258 [00:28<00:26,  4.73it/s]\u001b[A\nEpoch 1 Training:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 133/258 [00:28<00:26,  4.73it/s]\u001b[A\nEpoch 1 Training:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 134/258 [00:28<00:26,  4.74it/s]\u001b[A\nEpoch 1 Training:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 135/258 [00:29<00:25,  4.74it/s]\u001b[A\nEpoch 1 Training:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 136/258 [00:29<00:25,  4.74it/s]\u001b[A\nEpoch 1 Training:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 137/258 [00:29<00:25,  4.74it/s]\u001b[A\nEpoch 1 Training:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 138/258 [00:29<00:25,  4.74it/s]\u001b[A\nEpoch 1 Training:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 139/258 [00:30<00:25,  4.74it/s]\u001b[A\nEpoch 1 Training:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 140/258 [00:30<00:24,  4.73it/s]\u001b[A\nEpoch 1 Training:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 141/258 [00:30<00:24,  4.73it/s]\u001b[A\nEpoch 1 Training:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 142/258 [00:30<00:24,  4.73it/s]\u001b[A\nEpoch 1 Training:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 143/258 [00:30<00:24,  4.74it/s]\u001b[A\nEpoch 1 Training:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 144/258 [00:31<00:24,  4.72it/s]\u001b[A\nEpoch 1 Training:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 145/258 [00:31<00:23,  4.72it/s]\u001b[A\nEpoch 1 Training:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 146/258 [00:31<00:23,  4.69it/s]\u001b[A\nEpoch 1 Training:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 147/258 [00:31<00:23,  4.68it/s]\u001b[A\nEpoch 1 Training:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 148/258 [00:31<00:23,  4.70it/s]\u001b[A\nEpoch 1 Training:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 149/258 [00:32<00:23,  4.71it/s]\u001b[A\nEpoch 1 Training:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 150/258 [00:32<00:22,  4.72it/s]\u001b[A\nEpoch 1 Training:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 151/258 [00:32<00:22,  4.71it/s]\u001b[A\nEpoch 1 Training:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 152/258 [00:32<00:22,  4.71it/s]\u001b[A\nEpoch 1 Training:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 153/258 [00:33<00:22,  4.69it/s]\u001b[A\nEpoch 1 Training:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 154/258 [00:33<00:22,  4.67it/s]\u001b[A\nEpoch 1 Training:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 155/258 [00:33<00:22,  4.68it/s]\u001b[A\nEpoch 1 Training:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 156/258 [00:33<00:21,  4.68it/s]\u001b[A\nEpoch 1 Training:  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 157/258 [00:33<00:21,  4.68it/s]\u001b[A\nEpoch 1 Training:  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 158/258 [00:34<00:21,  4.69it/s]\u001b[A\nEpoch 1 Training:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 159/258 [00:34<00:21,  4.69it/s]\u001b[A\nEpoch 1 Training:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 160/258 [00:34<00:20,  4.70it/s]\u001b[A\nEpoch 1 Training:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 161/258 [00:34<00:20,  4.71it/s]\u001b[A\nEpoch 1 Training:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 162/258 [00:34<00:20,  4.72it/s]\u001b[A\nEpoch 1 Training:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 163/258 [00:35<00:20,  4.73it/s]\u001b[A\nEpoch 1 Training:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 164/258 [00:35<00:19,  4.73it/s]\u001b[A\nEpoch 1 Training:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 165/258 [00:35<00:19,  4.73it/s]\u001b[A\nEpoch 1 Training:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 166/258 [00:35<00:19,  4.74it/s]\u001b[A\nEpoch 1 Training:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 167/258 [00:35<00:19,  4.74it/s]\u001b[A\nEpoch 1 Training:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 168/258 [00:36<00:19,  4.73it/s]\u001b[A\nEpoch 1 Training:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 169/258 [00:36<00:18,  4.73it/s]\u001b[A\nEpoch 1 Training:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 170/258 [00:36<00:18,  4.74it/s]\u001b[A\nEpoch 1 Training:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 171/258 [00:36<00:18,  4.73it/s]\u001b[A\nEpoch 1 Training:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 172/258 [00:37<00:18,  4.73it/s]\u001b[A\nEpoch 1 Training:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 173/258 [00:37<00:17,  4.73it/s]\u001b[A\nEpoch 1 Training:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 174/258 [00:37<00:17,  4.73it/s]\u001b[A\nEpoch 1 Training:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 175/258 [00:37<00:17,  4.73it/s]\u001b[A\nEpoch 1 Training:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 176/258 [00:37<00:17,  4.74it/s]\u001b[A\nEpoch 1 Training:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 177/258 [00:38<00:17,  4.71it/s]\u001b[A\nEpoch 1 Training:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 178/258 [00:38<00:16,  4.73it/s]\u001b[A\nEpoch 1 Training:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 179/258 [00:38<00:16,  4.73it/s]\u001b[A\nEpoch 1 Training:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 180/258 [00:38<00:16,  4.74it/s]\u001b[A\nEpoch 1 Training:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 181/258 [00:38<00:16,  4.74it/s]\u001b[A\nEpoch 1 Training:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 182/258 [00:39<00:16,  4.74it/s]\u001b[A\nEpoch 1 Training:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 183/258 [00:39<00:15,  4.74it/s]\u001b[A\nEpoch 1 Training:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 184/258 [00:39<00:15,  4.73it/s]\u001b[A\nEpoch 1 Training:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 185/258 [00:39<00:15,  4.73it/s]\u001b[A\nEpoch 1 Training:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 186/258 [00:39<00:15,  4.74it/s]\u001b[A\nEpoch 1 Training:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 187/258 [00:40<00:15,  4.73it/s]\u001b[A\nEpoch 1 Training:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 188/258 [00:40<00:14,  4.74it/s]\u001b[A\nEpoch 1 Training:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 189/258 [00:40<00:14,  4.74it/s]\u001b[A\nEpoch 1 Training:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 190/258 [00:40<00:14,  4.73it/s]\u001b[A\nEpoch 1 Training:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 191/258 [00:41<00:14,  4.74it/s]\u001b[A\nEpoch 1 Training:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 192/258 [00:41<00:13,  4.75it/s]\u001b[A\nEpoch 1 Training:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 193/258 [00:41<00:13,  4.75it/s]\u001b[A\nEpoch 1 Training:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 194/258 [00:41<00:13,  4.75it/s]\u001b[A\nEpoch 1 Training:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 195/258 [00:41<00:13,  4.74it/s]\u001b[A\nEpoch 1 Training:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 196/258 [00:42<00:13,  4.74it/s]\u001b[A\nEpoch 1 Training:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 197/258 [00:42<00:12,  4.72it/s]\u001b[A\nEpoch 1 Training:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 198/258 [00:42<00:12,  4.73it/s]\u001b[A\nEpoch 1 Training:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 199/258 [00:42<00:12,  4.72it/s]\u001b[A\nEpoch 1 Training:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 200/258 [00:42<00:12,  4.72it/s]\u001b[A\nEpoch 1 Training:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 201/258 [00:43<00:12,  4.73it/s]\u001b[A\nEpoch 1 Training:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 202/258 [00:43<00:11,  4.73it/s]\u001b[A\nEpoch 1 Training:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 203/258 [00:43<00:11,  4.74it/s]\u001b[A\nEpoch 1 Training:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 204/258 [00:43<00:11,  4.73it/s]\u001b[A\nEpoch 1 Training:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 205/258 [00:44<00:11,  4.74it/s]\u001b[A\nEpoch 1 Training:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 206/258 [00:44<00:10,  4.74it/s]\u001b[A\nEpoch 1 Training:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 207/258 [00:44<00:10,  4.75it/s]\u001b[A\nEpoch 1 Training:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 208/258 [00:44<00:10,  4.75it/s]\u001b[A\nEpoch 1 Training:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 209/258 [00:44<00:10,  4.74it/s]\u001b[A\nEpoch 1 Training:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 210/258 [00:45<00:10,  4.73it/s]\u001b[A\nEpoch 1 Training:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 211/258 [00:45<00:09,  4.73it/s]\u001b[A\nEpoch 1 Training:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 212/258 [00:45<00:09,  4.73it/s]\u001b[A\nEpoch 1 Training:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 213/258 [00:45<00:09,  4.69it/s]\u001b[A\nEpoch 1 Training:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 214/258 [00:45<00:09,  4.68it/s]\u001b[A\nEpoch 1 Training:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 215/258 [00:46<00:09,  4.71it/s]\u001b[A\nEpoch 1 Training:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 216/258 [00:46<00:08,  4.72it/s]\u001b[A\nEpoch 1 Training:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 217/258 [00:46<00:08,  4.73it/s]\u001b[A\nEpoch 1 Training:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 218/258 [00:46<00:08,  4.74it/s]\u001b[A\nEpoch 1 Training:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 219/258 [00:46<00:08,  4.74it/s]\u001b[A\nEpoch 1 Training:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 220/258 [00:47<00:08,  4.74it/s]\u001b[A\nEpoch 1 Training:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 221/258 [00:47<00:07,  4.75it/s]\u001b[A\nEpoch 1 Training:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 222/258 [00:47<00:07,  4.73it/s]\u001b[A\nEpoch 1 Training:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 223/258 [00:47<00:07,  4.74it/s]\u001b[A\nEpoch 1 Training:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 224/258 [00:48<00:07,  4.73it/s]\u001b[A\nEpoch 1 Training:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 225/258 [00:48<00:06,  4.73it/s]\u001b[A\nEpoch 1 Training:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 226/258 [00:48<00:06,  4.73it/s]\u001b[A\nEpoch 1 Training:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 227/258 [00:48<00:06,  4.73it/s]\u001b[A\nEpoch 1 Training:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 228/258 [00:48<00:06,  4.74it/s]\u001b[A\nEpoch 1 Training:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 229/258 [00:49<00:06,  4.75it/s]\u001b[A\nEpoch 1 Training:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 230/258 [00:49<00:05,  4.74it/s]\u001b[A\nEpoch 1 Training:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 231/258 [00:49<00:05,  4.74it/s]\u001b[A\nEpoch 1 Training:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 232/258 [00:49<00:05,  4.74it/s]\u001b[A\nEpoch 1 Training:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 233/258 [00:49<00:05,  4.74it/s]\u001b[A\nEpoch 1 Training:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 234/258 [00:50<00:05,  4.73it/s]\u001b[A\nEpoch 1 Training:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 235/258 [00:50<00:04,  4.74it/s]\u001b[A\nEpoch 1 Training:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 236/258 [00:50<00:04,  4.74it/s]\u001b[A\nEpoch 1 Training:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 237/258 [00:50<00:04,  4.74it/s]\u001b[A\nEpoch 1 Training:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 238/258 [00:50<00:04,  4.74it/s]\u001b[A\nEpoch 1 Training:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 239/258 [00:51<00:04,  4.74it/s]\u001b[A\nEpoch 1 Training:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 240/258 [00:51<00:03,  4.74it/s]\u001b[A\nEpoch 1 Training:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 241/258 [00:51<00:03,  4.74it/s]\u001b[A\nEpoch 1 Training:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 242/258 [00:51<00:03,  4.74it/s]\u001b[A\nEpoch 1 Training:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 243/258 [00:52<00:03,  4.74it/s]\u001b[A\nEpoch 1 Training:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 244/258 [00:52<00:02,  4.74it/s]\u001b[A\nEpoch 1 Training:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 245/258 [00:52<00:02,  4.73it/s]\u001b[A\nEpoch 1 Training:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 246/258 [00:52<00:02,  4.73it/s]\u001b[A\nEpoch 1 Training:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 247/258 [00:52<00:02,  4.73it/s]\u001b[A\nEpoch 1 Training:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 248/258 [00:53<00:02,  4.73it/s]\u001b[A\nEpoch 1 Training:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 249/258 [00:53<00:01,  4.74it/s]\u001b[A\nEpoch 1 Training:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 250/258 [00:53<00:01,  4.74it/s]\u001b[A\nEpoch 1 Training:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 251/258 [00:53<00:01,  4.74it/s]\u001b[A\nEpoch 1 Training:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 252/258 [00:53<00:01,  4.73it/s]\u001b[A\nEpoch 1 Training:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 253/258 [00:54<00:01,  4.73it/s]\u001b[A\nEpoch 1 Training:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 254/258 [00:54<00:00,  4.73it/s]\u001b[A\nEpoch 1 Training:  99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 255/258 [00:54<00:00,  4.74it/s]\u001b[A\nEpoch 1 Training:  99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 256/258 [00:54<00:00,  4.73it/s]\u001b[A\nEpoch 1 Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 258/258 [00:55<00:00,  4.69it/s]\u001b[A\n","output_type":"stream"},{"name":"stdout","text":"Epoch [1/2], Train Loss: 0.1055, Acc: 0.9655, Time: 55.07s\nEpoch [1/2], Val   Loss: 0.0988, Acc: 0.9699, Time: 4.01s\nNew best model saved at epoch 1 with val loss: 0.0988\n","output_type":"stream"},{"name":"stderr","text":"Epoch 2 Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 258/258 [00:54<00:00,  4.77it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch [2/2], Train Loss: 0.0565, Acc: 0.9847, Time: 54.04s\nEpoch [2/2], Val   Loss: 0.0936, Acc: 0.9776, Time: 3.99s\nNew best model saved at epoch 2 with val loss: 0.0936\n\nâœ… Total training + validation time: 119.07 seconds\n\n","output_type":"stream"},{"name":"stderr","text":"Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\nEvaluating Text Model: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 44/44 [00:02<00:00, 16.15it/s]\n","output_type":"stream"},{"name":"stdout","text":"ðŸ§ª Test time: 2.73 seconds\n\n----- Results for Experiment 1: bs16_lr5e-05_ep2 -----\nBest epoch: 2 with validation loss: 0.0936\nTest Metrics:\n  Accuracy: 0.9857\n  Precision: 0.9775\n  Recall: 0.9943\n  F1 Score: 0.9858\n\n\n===== RUNNING EXPERIMENT 2/18 =====\nBatch Size: 16, Learning Rate: 5e-05, Epochs: 3\n\n===== STARTING EXPERIMENT: bs16_lr5e-05_ep3 =====\n\n","output_type":"stream"},{"name":"stderr","text":"Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\nEpoch 1 Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 258/258 [00:54<00:00,  4.77it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch [1/3], Train Loss: 0.1197, Acc: 0.9626, Time: 54.08s\nEpoch [1/3], Val   Loss: 0.2134, Acc: 0.9329, Time: 4.00s\nNew best model saved at epoch 1 with val loss: 0.2134\n","output_type":"stream"},{"name":"stderr","text":"Epoch 2 Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 258/258 [00:54<00:00,  4.77it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch [2/3], Train Loss: 0.0449, Acc: 0.9866, Time: 54.06s\nEpoch [2/3], Val   Loss: 0.0652, Acc: 0.9776, Time: 4.01s\nNew best model saved at epoch 2 with val loss: 0.0652\n","output_type":"stream"},{"name":"stderr","text":"Epoch 3 Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 258/258 [00:54<00:00,  4.77it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch [3/3], Train Loss: 0.0321, Acc: 0.9910, Time: 54.05s\nEpoch [3/3], Val   Loss: 0.1962, Acc: 0.9534, Time: 4.01s\n\nâœ… Total training + validation time: 176.14 seconds\n\n","output_type":"stream"},{"name":"stderr","text":"Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\nEvaluating Text Model: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 44/44 [00:02<00:00, 16.11it/s]\n","output_type":"stream"},{"name":"stdout","text":"ðŸ§ª Test time: 2.74 seconds\n\n----- Results for Experiment 2: bs16_lr5e-05_ep3 -----\nBest epoch: 2 with validation loss: 0.0652\nTest Metrics:\n  Accuracy: 0.9943\n  Precision: 0.9943\n  Recall: 0.9943\n  F1 Score: 0.9943\n\n\n===== RUNNING EXPERIMENT 3/18 =====\nBatch Size: 16, Learning Rate: 5e-05, Epochs: 4\n\n===== STARTING EXPERIMENT: bs16_lr5e-05_ep4 =====\n\n","output_type":"stream"},{"name":"stderr","text":"Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\nEpoch 1 Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 258/258 [00:54<00:00,  4.77it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch [1/4], Train Loss: 0.1113, Acc: 0.9628, Time: 54.08s\nEpoch [1/4], Val   Loss: 0.0629, Acc: 0.9796, Time: 4.05s\nNew best model saved at epoch 1 with val loss: 0.0629\n","output_type":"stream"},{"name":"stderr","text":"Epoch 2 Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 258/258 [00:54<00:00,  4.77it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch [2/4], Train Loss: 0.0322, Acc: 0.9910, Time: 54.08s\nEpoch [2/4], Val   Loss: 0.0604, Acc: 0.9786, Time: 4.03s\nNew best model saved at epoch 2 with val loss: 0.0604\n","output_type":"stream"},{"name":"stderr","text":"Epoch 3 Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 258/258 [00:54<00:00,  4.77it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch [3/4], Train Loss: 0.0307, Acc: 0.9913, Time: 54.14s\nEpoch [3/4], Val   Loss: 0.0987, Acc: 0.9660, Time: 4.03s\n","output_type":"stream"},{"name":"stderr","text":"Epoch 4 Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 258/258 [00:54<00:00,  4.77it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch [4/4], Train Loss: 0.0386, Acc: 0.9913, Time: 54.09s\nEpoch [4/4], Val   Loss: 0.1239, Acc: 0.9747, Time: 4.02s\n\nâœ… Total training + validation time: 234.49 seconds\n\n","output_type":"stream"},{"name":"stderr","text":"Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\nEvaluating Text Model: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 44/44 [00:02<00:00, 16.03it/s]\n","output_type":"stream"},{"name":"stdout","text":"ðŸ§ª Test time: 2.75 seconds\n\n----- Results for Experiment 3: bs16_lr5e-05_ep4 -----\nBest epoch: 2 with validation loss: 0.0604\nTest Metrics:\n  Accuracy: 0.9871\n  Precision: 0.9776\n  Recall: 0.9971\n  F1 Score: 0.9873\n\n\n===== RUNNING EXPERIMENT 4/18 =====\nBatch Size: 16, Learning Rate: 3e-05, Epochs: 2\n\n===== STARTING EXPERIMENT: bs16_lr3e-05_ep2 =====\n\n","output_type":"stream"},{"name":"stderr","text":"Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\nEpoch 1 Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 258/258 [00:54<00:00,  4.77it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch [1/2], Train Loss: 0.1160, Acc: 0.9587, Time: 54.12s\nEpoch [1/2], Val   Loss: 0.0745, Acc: 0.9776, Time: 4.01s\nNew best model saved at epoch 1 with val loss: 0.0745\n","output_type":"stream"},{"name":"stderr","text":"Epoch 2 Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 258/258 [00:54<00:00,  4.77it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch [2/2], Train Loss: 0.0432, Acc: 0.9878, Time: 54.06s\nEpoch [2/2], Val   Loss: 0.0529, Acc: 0.9825, Time: 4.03s\nNew best model saved at epoch 2 with val loss: 0.0529\n\nâœ… Total training + validation time: 118.23 seconds\n\n","output_type":"stream"},{"name":"stderr","text":"Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\nEvaluating Text Model: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 44/44 [00:02<00:00, 16.01it/s]\n","output_type":"stream"},{"name":"stdout","text":"ðŸ§ª Test time: 2.76 seconds\n\n----- Results for Experiment 4: bs16_lr3e-05_ep2 -----\nBest epoch: 2 with validation loss: 0.0529\nTest Metrics:\n  Accuracy: 0.9786\n  Precision: 0.9614\n  Recall: 0.9971\n  F1 Score: 0.9790\n\n\n===== RUNNING EXPERIMENT 5/18 =====\nBatch Size: 16, Learning Rate: 3e-05, Epochs: 3\n\n===== STARTING EXPERIMENT: bs16_lr3e-05_ep3 =====\n\n","output_type":"stream"},{"name":"stderr","text":"Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\nEpoch 1 Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 258/258 [00:54<00:00,  4.77it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch [1/3], Train Loss: 0.1020, Acc: 0.9621, Time: 54.05s\nEpoch [1/3], Val   Loss: 0.0600, Acc: 0.9825, Time: 4.01s\nNew best model saved at epoch 1 with val loss: 0.0600\n","output_type":"stream"},{"name":"stderr","text":"Epoch 2 Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 258/258 [00:54<00:00,  4.77it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch [2/3], Train Loss: 0.0392, Acc: 0.9903, Time: 54.06s\nEpoch [2/3], Val   Loss: 0.0522, Acc: 0.9825, Time: 4.00s\nNew best model saved at epoch 2 with val loss: 0.0522\n","output_type":"stream"},{"name":"stderr","text":"Epoch 3 Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 258/258 [00:54<00:00,  4.78it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch [3/3], Train Loss: 0.0285, Acc: 0.9939, Time: 54.02s\nEpoch [3/3], Val   Loss: 0.0770, Acc: 0.9825, Time: 4.01s\n\nâœ… Total training + validation time: 176.08 seconds\n\n","output_type":"stream"},{"name":"stderr","text":"Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\nEvaluating Text Model: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 44/44 [00:02<00:00, 16.11it/s]\n","output_type":"stream"},{"name":"stdout","text":"ðŸ§ª Test time: 2.74 seconds\n\n----- Results for Experiment 5: bs16_lr3e-05_ep3 -----\nBest epoch: 2 with validation loss: 0.0522\nTest Metrics:\n  Accuracy: 0.9886\n  Precision: 0.9803\n  Recall: 0.9971\n  F1 Score: 0.9887\n\n\n===== RUNNING EXPERIMENT 6/18 =====\nBatch Size: 16, Learning Rate: 3e-05, Epochs: 4\n\n===== STARTING EXPERIMENT: bs16_lr3e-05_ep4 =====\n\n","output_type":"stream"},{"name":"stderr","text":"Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\nEpoch 1 Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 258/258 [00:54<00:00,  4.77it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch [1/4], Train Loss: 0.1021, Acc: 0.9638, Time: 54.04s\nEpoch [1/4], Val   Loss: 0.0982, Acc: 0.9660, Time: 4.02s\nNew best model saved at epoch 1 with val loss: 0.0982\n","output_type":"stream"},{"name":"stderr","text":"Epoch 2 Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 258/258 [00:54<00:00,  4.77it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch [2/4], Train Loss: 0.0386, Acc: 0.9874, Time: 54.05s\nEpoch [2/4], Val   Loss: 0.1271, Acc: 0.9728, Time: 4.00s\n","output_type":"stream"},{"name":"stderr","text":"Epoch 3 Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 258/258 [00:54<00:00,  4.77it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch [3/4], Train Loss: 0.0190, Acc: 0.9942, Time: 54.07s\nEpoch [3/4], Val   Loss: 0.0572, Acc: 0.9835, Time: 4.03s\nNew best model saved at epoch 3 with val loss: 0.0572\n","output_type":"stream"},{"name":"stderr","text":"Epoch 4 Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 258/258 [00:54<00:00,  4.77it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch [4/4], Train Loss: 0.0171, Acc: 0.9939, Time: 54.11s\nEpoch [4/4], Val   Loss: 0.0903, Acc: 0.9786, Time: 4.02s\n\nâœ… Total training + validation time: 234.31 seconds\n\n","output_type":"stream"},{"name":"stderr","text":"Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\nEvaluating Text Model: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 44/44 [00:02<00:00, 16.03it/s]\n","output_type":"stream"},{"name":"stdout","text":"ðŸ§ª Test time: 2.75 seconds\n\n----- Results for Experiment 6: bs16_lr3e-05_ep4 -----\nBest epoch: 3 with validation loss: 0.0572\nTest Metrics:\n  Accuracy: 0.9871\n  Precision: 0.9749\n  Recall: 1.0000\n  F1 Score: 0.9873\n\n\n===== RUNNING EXPERIMENT 7/18 =====\nBatch Size: 16, Learning Rate: 2e-05, Epochs: 2\n\n===== STARTING EXPERIMENT: bs16_lr2e-05_ep2 =====\n\n","output_type":"stream"},{"name":"stderr","text":"Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\nEpoch 1 Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 258/258 [00:54<00:00,  4.77it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch [1/2], Train Loss: 0.1035, Acc: 0.9621, Time: 54.10s\nEpoch [1/2], Val   Loss: 0.0660, Acc: 0.9825, Time: 4.03s\nNew best model saved at epoch 1 with val loss: 0.0660\n","output_type":"stream"},{"name":"stderr","text":"Epoch 2 Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 258/258 [00:54<00:00,  4.77it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch [2/2], Train Loss: 0.0332, Acc: 0.9898, Time: 54.11s\nEpoch [2/2], Val   Loss: 0.0566, Acc: 0.9806, Time: 4.03s\nNew best model saved at epoch 2 with val loss: 0.0566\n\nâœ… Total training + validation time: 118.21 seconds\n\n","output_type":"stream"},{"name":"stderr","text":"Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\nEvaluating Text Model: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 44/44 [00:02<00:00, 16.02it/s]\n","output_type":"stream"},{"name":"stdout","text":"ðŸ§ª Test time: 2.76 seconds\n\n----- Results for Experiment 7: bs16_lr2e-05_ep2 -----\nBest epoch: 2 with validation loss: 0.0566\nTest Metrics:\n  Accuracy: 0.9814\n  Precision: 0.9668\n  Recall: 0.9971\n  F1 Score: 0.9817\n\n\n===== RUNNING EXPERIMENT 8/18 =====\nBatch Size: 16, Learning Rate: 2e-05, Epochs: 3\n\n===== STARTING EXPERIMENT: bs16_lr2e-05_ep3 =====\n\n","output_type":"stream"},{"name":"stderr","text":"Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\nEpoch 1 Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 258/258 [00:54<00:00,  4.77it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch [1/3], Train Loss: 0.1075, Acc: 0.9667, Time: 54.08s\nEpoch [1/3], Val   Loss: 0.0585, Acc: 0.9825, Time: 4.03s\nNew best model saved at epoch 1 with val loss: 0.0585\n","output_type":"stream"},{"name":"stderr","text":"Epoch 2 Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 258/258 [00:54<00:00,  4.77it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch [2/3], Train Loss: 0.0380, Acc: 0.9893, Time: 54.11s\nEpoch [2/3], Val   Loss: 0.0692, Acc: 0.9767, Time: 4.02s\n","output_type":"stream"},{"name":"stderr","text":"Epoch 3 Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 258/258 [00:54<00:00,  4.77it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch [3/3], Train Loss: 0.0162, Acc: 0.9956, Time: 54.06s\nEpoch [3/3], Val   Loss: 0.1215, Acc: 0.9679, Time: 4.01s\n\nâœ… Total training + validation time: 175.04 seconds\n\n","output_type":"stream"},{"name":"stderr","text":"Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\nEvaluating Text Model: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 44/44 [00:02<00:00, 16.08it/s]\n","output_type":"stream"},{"name":"stdout","text":"ðŸ§ª Test time: 2.75 seconds\n\n----- Results for Experiment 8: bs16_lr2e-05_ep3 -----\nBest epoch: 1 with validation loss: 0.0585\nTest Metrics:\n  Accuracy: 0.9814\n  Precision: 0.9642\n  Recall: 1.0000\n  F1 Score: 0.9818\n\n\n===== RUNNING EXPERIMENT 9/18 =====\nBatch Size: 16, Learning Rate: 2e-05, Epochs: 4\n\n===== STARTING EXPERIMENT: bs16_lr2e-05_ep4 =====\n\n","output_type":"stream"},{"name":"stderr","text":"Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\nEpoch 1 Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 258/258 [00:54<00:00,  4.77it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch [1/4], Train Loss: 0.1089, Acc: 0.9611, Time: 54.06s\nEpoch [1/4], Val   Loss: 0.0640, Acc: 0.9767, Time: 4.02s\nNew best model saved at epoch 1 with val loss: 0.0640\n","output_type":"stream"},{"name":"stderr","text":"Epoch 2 Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 258/258 [00:54<00:00,  4.77it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch [2/4], Train Loss: 0.0359, Acc: 0.9893, Time: 54.04s\nEpoch [2/4], Val   Loss: 0.0609, Acc: 0.9815, Time: 4.02s\nNew best model saved at epoch 2 with val loss: 0.0609\n","output_type":"stream"},{"name":"stderr","text":"Epoch 3 Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 258/258 [00:54<00:00,  4.77it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch [3/4], Train Loss: 0.0145, Acc: 0.9949, Time: 54.09s\nEpoch [3/4], Val   Loss: 0.1524, Acc: 0.9670, Time: 4.02s\n","output_type":"stream"},{"name":"stderr","text":"Epoch 4 Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 258/258 [00:54<00:00,  4.77it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch [4/4], Train Loss: 0.0143, Acc: 0.9959, Time: 54.10s\nEpoch [4/4], Val   Loss: 0.1164, Acc: 0.9718, Time: 4.02s\n\nâœ… Total training + validation time: 234.34 seconds\n\n","output_type":"stream"},{"name":"stderr","text":"Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\nEvaluating Text Model: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 44/44 [00:02<00:00, 16.01it/s]\n","output_type":"stream"},{"name":"stdout","text":"ðŸ§ª Test time: 2.76 seconds\n\n----- Results for Experiment 9: bs16_lr2e-05_ep4 -----\nBest epoch: 2 with validation loss: 0.0609\nTest Metrics:\n  Accuracy: 0.9886\n  Precision: 0.9831\n  Recall: 0.9943\n  F1 Score: 0.9886\n\n\n===== RUNNING EXPERIMENT 10/18 =====\nBatch Size: 32, Learning Rate: 5e-05, Epochs: 2\n\n===== STARTING EXPERIMENT: bs32_lr5e-05_ep2 =====\n\n","output_type":"stream"},{"name":"stderr","text":"Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\nEpoch 1 Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 129/129 [00:51<00:00,  2.49it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch [1/2], Train Loss: 0.1117, Acc: 0.9623, Time: 51.80s\nEpoch [1/2], Val   Loss: 0.0972, Acc: 0.9708, Time: 3.95s\nNew best model saved at epoch 1 with val loss: 0.0972\n","output_type":"stream"},{"name":"stderr","text":"Epoch 2 Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 129/129 [00:51<00:00,  2.50it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch [2/2], Train Loss: 0.0501, Acc: 0.9864, Time: 51.69s\nEpoch [2/2], Val   Loss: 0.0584, Acc: 0.9815, Time: 3.94s\nNew best model saved at epoch 2 with val loss: 0.0584\n\nâœ… Total training + validation time: 113.33 seconds\n\n","output_type":"stream"},{"name":"stderr","text":"Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\nEvaluating Text Model: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 22/22 [00:02<00:00,  8.11it/s]\n","output_type":"stream"},{"name":"stdout","text":"ðŸ§ª Test time: 2.72 seconds\n\n----- Results for Experiment 10: bs32_lr5e-05_ep2 -----\nBest epoch: 2 with validation loss: 0.0584\nTest Metrics:\n  Accuracy: 0.9829\n  Precision: 0.9829\n  Recall: 0.9829\n  F1 Score: 0.9829\n\n\n===== RUNNING EXPERIMENT 11/18 =====\nBatch Size: 32, Learning Rate: 5e-05, Epochs: 3\n\n===== STARTING EXPERIMENT: bs32_lr5e-05_ep3 =====\n\n","output_type":"stream"},{"name":"stderr","text":"Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\nEpoch 1 Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 129/129 [00:51<00:00,  2.50it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch [1/3], Train Loss: 0.1145, Acc: 0.9623, Time: 51.63s\nEpoch [1/3], Val   Loss: 0.0921, Acc: 0.9699, Time: 3.96s\nNew best model saved at epoch 1 with val loss: 0.0921\n","output_type":"stream"},{"name":"stderr","text":"Epoch 2 Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 129/129 [00:51<00:00,  2.50it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch [2/3], Train Loss: 0.0423, Acc: 0.9874, Time: 51.70s\nEpoch [2/3], Val   Loss: 0.0623, Acc: 0.9796, Time: 3.97s\nNew best model saved at epoch 2 with val loss: 0.0623\n","output_type":"stream"},{"name":"stderr","text":"Epoch 3 Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 129/129 [00:51<00:00,  2.50it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch [3/3], Train Loss: 0.0219, Acc: 0.9942, Time: 51.68s\nEpoch [3/3], Val   Loss: 0.0655, Acc: 0.9796, Time: 3.96s\n\nâœ… Total training + validation time: 168.82 seconds\n\n","output_type":"stream"},{"name":"stderr","text":"Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\nEvaluating Text Model: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 22/22 [00:02<00:00,  8.10it/s]\n","output_type":"stream"},{"name":"stdout","text":"ðŸ§ª Test time: 2.73 seconds\n\n----- Results for Experiment 11: bs32_lr5e-05_ep3 -----\nBest epoch: 2 with validation loss: 0.0623\nTest Metrics:\n  Accuracy: 0.9857\n  Precision: 0.9749\n  Recall: 0.9971\n  F1 Score: 0.9859\n\n\n===== RUNNING EXPERIMENT 12/18 =====\nBatch Size: 32, Learning Rate: 5e-05, Epochs: 4\n\n===== STARTING EXPERIMENT: bs32_lr5e-05_ep4 =====\n\n","output_type":"stream"},{"name":"stderr","text":"Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\nEpoch 1 Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 129/129 [00:51<00:00,  2.49it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch [1/4], Train Loss: 0.1188, Acc: 0.9560, Time: 51.73s\nEpoch [1/4], Val   Loss: 0.0622, Acc: 0.9825, Time: 3.94s\nNew best model saved at epoch 1 with val loss: 0.0622\n","output_type":"stream"},{"name":"stderr","text":"Epoch 2 Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 129/129 [00:51<00:00,  2.50it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch [2/4], Train Loss: 0.0419, Acc: 0.9878, Time: 51.63s\nEpoch [2/4], Val   Loss: 0.0545, Acc: 0.9796, Time: 3.93s\nNew best model saved at epoch 2 with val loss: 0.0545\n","output_type":"stream"},{"name":"stderr","text":"Epoch 3 Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 129/129 [00:51<00:00,  2.50it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch [3/4], Train Loss: 0.0159, Acc: 0.9959, Time: 51.69s\nEpoch [3/4], Val   Loss: 0.0677, Acc: 0.9796, Time: 3.93s\n","output_type":"stream"},{"name":"stderr","text":"Epoch 4 Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 129/129 [00:51<00:00,  2.50it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch [4/4], Train Loss: 0.0123, Acc: 0.9971, Time: 51.64s\nEpoch [4/4], Val   Loss: 0.1138, Acc: 0.9728, Time: 3.96s\n\nâœ… Total training + validation time: 224.41 seconds\n\n","output_type":"stream"},{"name":"stderr","text":"Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\nEvaluating Text Model: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 22/22 [00:02<00:00,  8.12it/s]\n","output_type":"stream"},{"name":"stdout","text":"ðŸ§ª Test time: 2.72 seconds\n\n----- Results for Experiment 12: bs32_lr5e-05_ep4 -----\nBest epoch: 2 with validation loss: 0.0545\nTest Metrics:\n  Accuracy: 0.9900\n  Precision: 0.9886\n  Recall: 0.9914\n  F1 Score: 0.9900\n\n\n===== RUNNING EXPERIMENT 13/18 =====\nBatch Size: 32, Learning Rate: 3e-05, Epochs: 2\n\n===== STARTING EXPERIMENT: bs32_lr3e-05_ep2 =====\n\n","output_type":"stream"},{"name":"stderr","text":"Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\nEpoch 1 Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 129/129 [00:51<00:00,  2.49it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch [1/2], Train Loss: 0.1187, Acc: 0.9614, Time: 51.71s\nEpoch [1/2], Val   Loss: 0.0706, Acc: 0.9776, Time: 3.96s\nNew best model saved at epoch 1 with val loss: 0.0706\n","output_type":"stream"},{"name":"stderr","text":"Epoch 2 Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 129/129 [00:51<00:00,  2.50it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch [2/2], Train Loss: 0.0409, Acc: 0.9898, Time: 51.62s\nEpoch [2/2], Val   Loss: 0.0642, Acc: 0.9767, Time: 3.94s\nNew best model saved at epoch 2 with val loss: 0.0642\n\nâœ… Total training + validation time: 113.19 seconds\n\n","output_type":"stream"},{"name":"stderr","text":"Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\nEvaluating Text Model: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 22/22 [00:02<00:00,  8.07it/s]\n","output_type":"stream"},{"name":"stdout","text":"ðŸ§ª Test time: 2.74 seconds\n\n----- Results for Experiment 13: bs32_lr3e-05_ep2 -----\nBest epoch: 2 with validation loss: 0.0642\nTest Metrics:\n  Accuracy: 0.9814\n  Precision: 0.9642\n  Recall: 1.0000\n  F1 Score: 0.9818\n\n\n===== RUNNING EXPERIMENT 14/18 =====\nBatch Size: 32, Learning Rate: 3e-05, Epochs: 3\n\n===== STARTING EXPERIMENT: bs32_lr3e-05_ep3 =====\n\n","output_type":"stream"},{"name":"stderr","text":"Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\nEpoch 1 Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 129/129 [00:51<00:00,  2.50it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch [1/3], Train Loss: 0.1088, Acc: 0.9626, Time: 51.70s\nEpoch [1/3], Val   Loss: 0.0573, Acc: 0.9815, Time: 3.94s\nNew best model saved at epoch 1 with val loss: 0.0573\n","output_type":"stream"},{"name":"stderr","text":"Epoch 2 Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 129/129 [00:51<00:00,  2.49it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch [2/3], Train Loss: 0.0308, Acc: 0.9927, Time: 51.74s\nEpoch [2/3], Val   Loss: 0.0561, Acc: 0.9825, Time: 3.94s\nNew best model saved at epoch 2 with val loss: 0.0561\n","output_type":"stream"},{"name":"stderr","text":"Epoch 3 Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 129/129 [00:51<00:00,  2.49it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch [3/3], Train Loss: 0.0125, Acc: 0.9978, Time: 51.72s\nEpoch [3/3], Val   Loss: 0.1097, Acc: 0.9767, Time: 3.94s\n\nâœ… Total training + validation time: 169.01 seconds\n\n","output_type":"stream"},{"name":"stderr","text":"Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\nEvaluating Text Model: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 22/22 [00:02<00:00,  8.13it/s]\n","output_type":"stream"},{"name":"stdout","text":"ðŸ§ª Test time: 2.72 seconds\n\n----- Results for Experiment 14: bs32_lr3e-05_ep3 -----\nBest epoch: 2 with validation loss: 0.0561\nTest Metrics:\n  Accuracy: 0.9871\n  Precision: 0.9858\n  Recall: 0.9886\n  F1 Score: 0.9872\n\n\n===== RUNNING EXPERIMENT 15/18 =====\nBatch Size: 32, Learning Rate: 3e-05, Epochs: 4\n\n===== STARTING EXPERIMENT: bs32_lr3e-05_ep4 =====\n\n","output_type":"stream"},{"name":"stderr","text":"Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\nEpoch 1 Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 129/129 [00:51<00:00,  2.49it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch [1/4], Train Loss: 0.1037, Acc: 0.9670, Time: 51.73s\nEpoch [1/4], Val   Loss: 0.0669, Acc: 0.9815, Time: 3.96s\nNew best model saved at epoch 1 with val loss: 0.0669\n","output_type":"stream"},{"name":"stderr","text":"Epoch 2 Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 129/129 [00:51<00:00,  2.49it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch [2/4], Train Loss: 0.0474, Acc: 0.9871, Time: 51.74s\nEpoch [2/4], Val   Loss: 0.0617, Acc: 0.9845, Time: 3.95s\nNew best model saved at epoch 2 with val loss: 0.0617\n","output_type":"stream"},{"name":"stderr","text":"Epoch 3 Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 129/129 [00:51<00:00,  2.50it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch [3/4], Train Loss: 0.0234, Acc: 0.9949, Time: 51.67s\nEpoch [3/4], Val   Loss: 0.0885, Acc: 0.9806, Time: 3.95s\n","output_type":"stream"},{"name":"stderr","text":"Epoch 4 Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 129/129 [00:51<00:00,  2.50it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch [4/4], Train Loss: 0.0132, Acc: 0.9973, Time: 51.69s\nEpoch [4/4], Val   Loss: 0.0686, Acc: 0.9806, Time: 3.95s\n\nâœ… Total training + validation time: 224.66 seconds\n\n","output_type":"stream"},{"name":"stderr","text":"Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\nEvaluating Text Model: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 22/22 [00:02<00:00,  8.14it/s]\n","output_type":"stream"},{"name":"stdout","text":"ðŸ§ª Test time: 2.71 seconds\n\n----- Results for Experiment 15: bs32_lr3e-05_ep4 -----\nBest epoch: 2 with validation loss: 0.0617\nTest Metrics:\n  Accuracy: 0.9900\n  Precision: 0.9914\n  Recall: 0.9886\n  F1 Score: 0.9900\n\n\n===== RUNNING EXPERIMENT 16/18 =====\nBatch Size: 32, Learning Rate: 2e-05, Epochs: 2\n\n===== STARTING EXPERIMENT: bs32_lr2e-05_ep2 =====\n\n","output_type":"stream"},{"name":"stderr","text":"Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\nEpoch 1 Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 129/129 [00:51<00:00,  2.50it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch [1/2], Train Loss: 0.1099, Acc: 0.9643, Time: 51.66s\nEpoch [1/2], Val   Loss: 0.0726, Acc: 0.9767, Time: 3.95s\nNew best model saved at epoch 1 with val loss: 0.0726\n","output_type":"stream"},{"name":"stderr","text":"Epoch 2 Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 129/129 [00:51<00:00,  2.49it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch [2/2], Train Loss: 0.0418, Acc: 0.9898, Time: 51.71s\nEpoch [2/2], Val   Loss: 0.0566, Acc: 0.9845, Time: 3.94s\nNew best model saved at epoch 2 with val loss: 0.0566\n\nâœ… Total training + validation time: 113.26 seconds\n\n","output_type":"stream"},{"name":"stderr","text":"Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\nEvaluating Text Model: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 22/22 [00:02<00:00,  8.11it/s]\n","output_type":"stream"},{"name":"stdout","text":"ðŸ§ª Test time: 2.72 seconds\n\n----- Results for Experiment 16: bs32_lr2e-05_ep2 -----\nBest epoch: 2 with validation loss: 0.0566\nTest Metrics:\n  Accuracy: 0.9857\n  Precision: 0.9802\n  Recall: 0.9914\n  F1 Score: 0.9858\n\n\n===== RUNNING EXPERIMENT 17/18 =====\nBatch Size: 32, Learning Rate: 2e-05, Epochs: 3\n\n===== STARTING EXPERIMENT: bs32_lr2e-05_ep3 =====\n\n","output_type":"stream"},{"name":"stderr","text":"Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\nEpoch 1 Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 129/129 [00:51<00:00,  2.49it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch [1/3], Train Loss: 0.1202, Acc: 0.9550, Time: 51.75s\nEpoch [1/3], Val   Loss: 0.0603, Acc: 0.9825, Time: 3.95s\nNew best model saved at epoch 1 with val loss: 0.0603\n","output_type":"stream"},{"name":"stderr","text":"Epoch 2 Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 129/129 [00:51<00:00,  2.49it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch [2/3], Train Loss: 0.0369, Acc: 0.9898, Time: 51.76s\nEpoch [2/3], Val   Loss: 0.0601, Acc: 0.9776, Time: 3.94s\nNew best model saved at epoch 2 with val loss: 0.0601\n","output_type":"stream"},{"name":"stderr","text":"Epoch 3 Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 129/129 [00:51<00:00,  2.50it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch [3/3], Train Loss: 0.0178, Acc: 0.9949, Time: 51.69s\nEpoch [3/3], Val   Loss: 0.0678, Acc: 0.9825, Time: 3.94s\n\nâœ… Total training + validation time: 169.01 seconds\n\n","output_type":"stream"},{"name":"stderr","text":"Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\nEvaluating Text Model: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 22/22 [00:02<00:00,  8.13it/s]\n","output_type":"stream"},{"name":"stdout","text":"ðŸ§ª Test time: 2.71 seconds\n\n----- Results for Experiment 17: bs32_lr2e-05_ep3 -----\nBest epoch: 2 with validation loss: 0.0601\nTest Metrics:\n  Accuracy: 0.9914\n  Precision: 0.9886\n  Recall: 0.9943\n  F1 Score: 0.9915\n\n\n===== RUNNING EXPERIMENT 18/18 =====\nBatch Size: 32, Learning Rate: 2e-05, Epochs: 4\n\n===== STARTING EXPERIMENT: bs32_lr2e-05_ep4 =====\n\n","output_type":"stream"},{"name":"stderr","text":"Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\nEpoch 1 Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 129/129 [00:51<00:00,  2.50it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch [1/4], Train Loss: 0.1243, Acc: 0.9565, Time: 51.69s\nEpoch [1/4], Val   Loss: 0.0621, Acc: 0.9776, Time: 3.95s\nNew best model saved at epoch 1 with val loss: 0.0621\n","output_type":"stream"},{"name":"stderr","text":"Epoch 2 Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 129/129 [00:51<00:00,  2.49it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch [2/4], Train Loss: 0.0390, Acc: 0.9891, Time: 51.74s\nEpoch [2/4], Val   Loss: 0.0563, Acc: 0.9806, Time: 3.95s\nNew best model saved at epoch 2 with val loss: 0.0563\n","output_type":"stream"},{"name":"stderr","text":"Epoch 3 Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 129/129 [00:51<00:00,  2.50it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch [3/4], Train Loss: 0.0197, Acc: 0.9954, Time: 51.67s\nEpoch [3/4], Val   Loss: 0.0488, Acc: 0.9874, Time: 3.95s\nNew best model saved at epoch 3 with val loss: 0.0488\n","output_type":"stream"},{"name":"stderr","text":"Epoch 4 Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 129/129 [00:51<00:00,  2.49it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch [4/4], Train Loss: 0.0180, Acc: 0.9959, Time: 51.76s\nEpoch [4/4], Val   Loss: 0.0652, Acc: 0.9815, Time: 3.94s\n\nâœ… Total training + validation time: 225.92 seconds\n\n","output_type":"stream"},{"name":"stderr","text":"Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\nEvaluating Text Model: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 22/22 [00:02<00:00,  8.15it/s]","output_type":"stream"},{"name":"stdout","text":"ðŸ§ª Test time: 2.71 seconds\n\n----- Results for Experiment 18: bs32_lr2e-05_ep4 -----\nBest epoch: 3 with validation loss: 0.0488\nTest Metrics:\n  Accuracy: 0.9886\n  Precision: 0.9831\n  Recall: 0.9943\n  F1 Score: 0.9886\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}],"execution_count":23},{"cell_type":"code","source":"# Create a DataFrame for easier analysis\nresults_df = pd.DataFrame([\n    {\n        'Experiment': r['experiment_name'],\n        'Batch Size': int(r['experiment_name'].split('_')[0][2:]),\n        'Learning Rate': float(r['experiment_name'].split('_')[1][2:]),\n        'Epochs': int(r['experiment_name'].split('_')[2][2:]),\n        'Best Epoch': r['best_epoch'],\n        'Best Val Loss': r['best_val_loss'],\n        'Accuracy': r['metrics']['Accuracy'],\n        'Precision': r['metrics']['Precision'],\n        'Recall': r['metrics']['Recall'],\n        'F1 Score': r['metrics']['F1 Score'],\n        'Model Path': r['model_path']\n    }\n    for r in results\n])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-09T05:09:10.873349Z","iopub.execute_input":"2025-05-09T05:09:10.873619Z","iopub.status.idle":"2025-05-09T05:09:10.880666Z","shell.execute_reply.started":"2025-05-09T05:09:10.873601Z","shell.execute_reply":"2025-05-09T05:09:10.879960Z"}},"outputs":[],"execution_count":24},{"cell_type":"code","source":"from datetime import datetime\n# Save results to CSV\ntimestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\nresults_csv_path = f'text_model_experiments_results_{timestamp}.csv'\nresults_df.to_csv(results_csv_path, index=False)\nprint(f\"\\nResults saved to {results_csv_path}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-09T05:09:10.881488Z","iopub.execute_input":"2025-05-09T05:09:10.881732Z","iopub.status.idle":"2025-05-09T05:09:10.919655Z","shell.execute_reply.started":"2025-05-09T05:09:10.881709Z","shell.execute_reply":"2025-05-09T05:09:10.918947Z"}},"outputs":[{"name":"stdout","text":"\nResults saved to text_model_experiments_results_20250509_050910.csv\n","output_type":"stream"}],"execution_count":25},{"cell_type":"code","source":"# Print summary of all experiment results sorted by F1 Score\nprint(\"\\n===== EXPERIMENT RESULTS SUMMARY (Sorted by F1 Score) =====\")\nsorted_results = results_df.sort_values('F1 Score', ascending=False)\nprint(results_df[['Experiment', 'Accuracy', 'Precision', 'Recall','F1 Score']])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-09T05:09:10.920418Z","iopub.execute_input":"2025-05-09T05:09:10.920901Z","iopub.status.idle":"2025-05-09T05:09:10.930561Z","shell.execute_reply.started":"2025-05-09T05:09:10.920877Z","shell.execute_reply":"2025-05-09T05:09:10.929901Z"}},"outputs":[{"name":"stdout","text":"\n===== EXPERIMENT RESULTS SUMMARY (Sorted by F1 Score) =====\n          Experiment  Accuracy  Precision    Recall  F1 Score\n0   bs16_lr5e-05_ep2  0.985714   0.977528  0.994286  0.985836\n1   bs16_lr5e-05_ep3  0.994286   0.994286  0.994286  0.994286\n2   bs16_lr5e-05_ep4  0.987143   0.977591  0.997143  0.987270\n3   bs16_lr3e-05_ep2  0.978571   0.961433  0.997143  0.978962\n4   bs16_lr3e-05_ep3  0.988571   0.980337  0.997143  0.988669\n5   bs16_lr3e-05_ep4  0.987143   0.974930  1.000000  0.987306\n6   bs16_lr2e-05_ep2  0.981429   0.966759  0.997143  0.981716\n7   bs16_lr2e-05_ep3  0.981429   0.964187  1.000000  0.981767\n8   bs16_lr2e-05_ep4  0.988571   0.983051  0.994286  0.988636\n9   bs32_lr5e-05_ep2  0.982857   0.982857  0.982857  0.982857\n10  bs32_lr5e-05_ep3  0.985714   0.974860  0.997143  0.985876\n11  bs32_lr5e-05_ep4  0.990000   0.988604  0.991429  0.990014\n12  bs32_lr3e-05_ep2  0.981429   0.964187  1.000000  0.981767\n13  bs32_lr3e-05_ep3  0.987143   0.985755  0.988571  0.987161\n14  bs32_lr3e-05_ep4  0.990000   0.991404  0.988571  0.989986\n15  bs32_lr2e-05_ep2  0.985714   0.980226  0.991429  0.985795\n16  bs32_lr2e-05_ep3  0.991429   0.988636  0.994286  0.991453\n17  bs32_lr2e-05_ep4  0.988571   0.983051  0.994286  0.988636\n","output_type":"stream"}],"execution_count":26},{"cell_type":"code","source":"# Find the best model based on F1 score\nbest_exp = sorted_results.iloc[0]\nprint(\"\\n===== BEST MODEL =====\")\nprint(f\"Configuration: {best_exp['Experiment']}\")\nprint(f\"Batch Size: {best_exp['Batch Size']}\")\nprint(f\"Learning Rate: {best_exp['Learning Rate']}\")\nprint(f\"Total Epochs: {best_exp['Epochs']}\")\nprint(f\"Best Epoch: {best_exp['Best Epoch']}\")\nprint(f\"F1 Score: {best_exp['F1 Score']:.4f}\")\nprint(f\"Accuracy: {best_exp['Accuracy']:.4f}\")\nprint(f\"Precision: {best_exp['Precision']:.4f}\")\nprint(f\"Recall: {best_exp['Recall']:.4f}\")\nprint(f\"Model Path: {best_exp['Model Path']}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-09T05:09:10.931297Z","iopub.execute_input":"2025-05-09T05:09:10.931797Z","iopub.status.idle":"2025-05-09T05:09:10.950472Z","shell.execute_reply.started":"2025-05-09T05:09:10.931773Z","shell.execute_reply":"2025-05-09T05:09:10.949897Z"}},"outputs":[{"name":"stdout","text":"\n===== BEST MODEL =====\nConfiguration: bs16_lr5e-05_ep3\nBatch Size: 16\nLearning Rate: 5e-05\nTotal Epochs: 3\nBest Epoch: 2\nF1 Score: 0.9943\nAccuracy: 0.9943\nPrecision: 0.9943\nRecall: 0.9943\nModel Path: best_text_model_bs16_lr5e-05_ep3_state_dict.pt\n","output_type":"stream"}],"execution_count":27},{"cell_type":"markdown","source":"# -----------------","metadata":{}}]}